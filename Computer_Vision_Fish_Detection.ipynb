{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPN4GsCpZC4W"
      },
      "source": [
        "# Preliminary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKNTf7L4b2z7"
      },
      "source": [
        "Here we install all imports and other necessary components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1H3gELF5H9o"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccCEcC7sI89U",
        "outputId": "69ccc84a-99bf-439e-fe3d-3cf05d39cd9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.2.80 ðŸš€ Python-3.10.1 torch-2.4.0 CPU (Apple M1)\n",
            "Setup complete âœ… (8 CPUs, 16.0 GB RAM, 351.4/460.4 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "!pip install imageio\n",
        "!pip install numpy\n",
        "!pip install opencv-python\n",
        "!pip install tqdm\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE2jk4zOmoPu",
        "outputId": "a9c5728a-4ed8-42a8-f8d6-a60268f21504"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MoiM3ryXiBG",
        "outputId": "b65b2bc9-d6c6-4d71-ebad-17cbdfd6177f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: nvidia-smi\n"
          ]
        }
      ],
      "source": [
        "# Check what GPU is available\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Mn3ayM5PUR"
      },
      "source": [
        "## Directories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "RafB5uuaOSMK"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\"/Users/jan/Documents/code/cv/project\")\n",
        "\n",
        "\"\"\"\n",
        "# Training set\n",
        "VIDEO_DIR = BASE_DIR / \"data/fishclef_2015_release/training_set/videos\"\n",
        "GT_DIR = BASE_DIR / \"data/fishclef_2015_release/training_set/gt\"\n",
        "IMG_DIR = BASE_DIR / \"train_img/\"\n",
        "GMM_DIR = BASE_DIR / \"train_gmm/\"\n",
        "OPTICAL_DIR = BASE_DIR / \"train_optical/\"\n",
        "GMM_OPTICAL_DIR = BASE_DIR / \"train_gmm_optical/\"\n",
        "\"\"\"\n",
        "\n",
        "# Test set\n",
        "BASE_DIR = Path(\"/Users/jan/Documents/code/cv/project\")\n",
        "VIDEO_DIR = BASE_DIR / \"data/fishclef_2015_release/test_set/videos\"\n",
        "GT_DIR = BASE_DIR / \"data/fishclef_2015_release/test_set/gt\"\n",
        "IMG_DIR = BASE_DIR / \"test_img/\"\n",
        "GMM_DIR = BASE_DIR / \"test_gmm/\"\n",
        "OPTICAL_DIR = BASE_DIR / \"test_optical/\"\n",
        "GMM_OPTICAL_DIR = BASE_DIR / \"test_gmm_optical/\"\n",
        "\n",
        "SPECIES_LIST = [\n",
        "    \"abudefduf vaigiensis\",\n",
        "    \"acanthurus nigrofuscus\",\n",
        "    \"amphiprion clarkii\",\n",
        "    \"chaetodon lununatus\",\n",
        "    \"chaetodon speculum\",\n",
        "    \"chaetodon trifascialis\",\n",
        "    \"chromis chrysura\",\n",
        "    \"dascyllus aruanus\",\n",
        "    \"dascyllus reticulatus\",\n",
        "    \"hemigumnus malapterus\",\n",
        "    \"myripristis kuntee\",\n",
        "    \"neoglyphidodon nigroris\",\n",
        "    \"pempheris vanicolensis\",\n",
        "    \"plectrogly-phidodon dickii\",\n",
        "    \"zebrasoma scopas\",\n",
        "]\n",
        "\n",
        "UNKNOWN_LABEL = 15\n",
        "\n",
        "FOREGROUND_DETECTOR_PARAMS = {\n",
        "    \"history\": 250,\n",
        "    \"varThreshold\": 16,\n",
        "    \"detectShadows\": True,\n",
        "}\n",
        "BLOB_ANALYSIS_PARAMS = {\"min_area\": 200}\n",
        "STRUCTURING_ELEMENT_OPEN = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "STRUCTURING_ELEMENT_CLOSE = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "FRAME_RESIZE = (640, 640)\n",
        "FRAME_ADJUST_GAMMA = 1.5\n",
        "FARNEBACK_PARAMS = {\n",
        "    \"pyr_scale\": 0.5,\n",
        "    \"levels\": 3,\n",
        "    \"winsize\": 15,\n",
        "    \"iterations\": 3,\n",
        "    \"poly_n\": 5,\n",
        "    \"poly_sigma\": 1.2,\n",
        "    \"flags\": 0,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMR9UH9jPyNX"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ymY_V2japXC"
      },
      "source": [
        "# Create training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1aWQB5tt0Req"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(IMG_DIR) == False:\n",
        "    os.mkdir(IMG_DIR)\n",
        "\n",
        "if os.path.exists(GMM_DIR) == False:\n",
        "    os.mkdir(GMM_DIR)\n",
        "\n",
        "if os.path.exists(OPTICAL_DIR) == False:\n",
        "    os.mkdir(OPTICAL_DIR)\n",
        "\n",
        "if os.path.exists(GMM_OPTICAL_DIR) == False:\n",
        "    os.mkdir(GMM_OPTICAL_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "pMM9grDh0b5q"
      },
      "outputs": [],
      "source": [
        "def adjust_gamma(image, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Adjust the gamma of an image.\n",
        "\n",
        "    Parameters:\n",
        "    - image (numpy.ndarray): The input image on which gamma correction is to be applied.\n",
        "    - gamma (float): The gamma value for correction. Default is 1.0. Values less than 1.0 will make the image darker,\n",
        "                     while values greater than 1.0 will make the image lighter.\n",
        "\n",
        "    Returns:\n",
        "    - numpy.ndarray: The gamma-corrected image.\n",
        "    \"\"\"\n",
        "    # Calculate the inverse of the gamma value\n",
        "    invGamma = 1.0 / gamma\n",
        "\n",
        "    # Build a lookup table mapping pixel values [0, 255] to their adjusted gamma values\n",
        "    table = np.array([(i / 255.0) ** invGamma * 255 for i in range(256)], dtype=\"uint8\")\n",
        "\n",
        "    # Apply the gamma correction using the lookup table\n",
        "    return cv2.LUT(image, table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_gmm_annotation(annotation_filename, bboxes, image_width, image_height):\n",
        "    \"\"\"\n",
        "    Save annotations in YOLO format for Gaussian Mixture Model (GMM) detected bounding boxes.\n",
        "\n",
        "    Parameters:\n",
        "    - annotation_filename (str): The file to save annotations.\n",
        "    - bboxes (list of tuples): List of bounding boxes, where each bounding box is represented as a tuple (x, y, width, height).\n",
        "    - image_width (int): Width of the image.\n",
        "    - image_height (int): Height of the image.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Open the annotation file in write mode\n",
        "    with open(annotation_filename, \"w\") as file:\n",
        "        # Iterate over each bounding box\n",
        "        for x, y, width, height in bboxes:\n",
        "            # Normalize the coordinates\n",
        "            x_center = (x + width / 2.0) / image_width\n",
        "            y_center = (y + height / 2.0) / image_height\n",
        "            width /= image_width\n",
        "            height /= image_height\n",
        "\n",
        "            # Write the normalized coordinates to the file in YOLO format\n",
        "            file.write(f\"0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def save_annotation_batch_test(\n",
        "    name, annotation_file_path, bboxes, image_width, image_height\n",
        "):\n",
        "    \"\"\"\n",
        "    Save annotations in YOLO format for each frame in batches.\n",
        "\n",
        "    Parameters:\n",
        "    - name (str): Base name for the annotation files.\n",
        "    - annotation_file_path (Path): Path object representing the directory to save annotation files.\n",
        "    - bboxes (list of dict): List of bounding boxes, where each bounding box is represented as a dictionary with keys 'frame_id', 'fish_species', 'x', 'y', 'w', 'h'.\n",
        "    - image_width (int): Width of the image.\n",
        "    - image_height (int): Height of the image.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Group bounding boxes by frame_id\n",
        "    frame_bboxes = {}\n",
        "    for bbox in bboxes:\n",
        "        frame_id = bbox[\"frame_id\"]\n",
        "        frame_bboxes.setdefault(frame_id, []).append(bbox)\n",
        "\n",
        "    # Prepare content for each file\n",
        "    file_contents = {}\n",
        "    for frame_id, bboxes in frame_bboxes.items():\n",
        "        # Collect annotation lines for this frame\n",
        "        content = []\n",
        "        for fish in bboxes:\n",
        "            fish_species = fish.get(\"species_name\", \"\").lower()\n",
        "            x, y, width, height = (\n",
        "                fish.get(\"x\", 0),\n",
        "                fish.get(\"y\", 0),\n",
        "                fish.get(\"w\", 0),\n",
        "                fish.get(\"h\", 0),\n",
        "            )\n",
        "\n",
        "            # Normalize the coordinates\n",
        "            x_center = (x + width / 2.0) / image_width\n",
        "            y_center = (y + height / 2.0) / image_height\n",
        "            width /= image_width\n",
        "            height /= image_height\n",
        "\n",
        "            # Determine the species index\n",
        "            species_index = (\n",
        "                SPECIES_LIST.index(fish_species)\n",
        "                if fish_species in SPECIES_LIST\n",
        "                else UNKNOWN_LABEL\n",
        "            )\n",
        "\n",
        "            # Format the annotation line in YOLO format\n",
        "            content.append(\n",
        "                f\"{species_index} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
        "            )\n",
        "\n",
        "        # Add content for the current frame\n",
        "        file_contents[frame_id] = \"\\n\".join(content)\n",
        "\n",
        "    # Write all files in a batch\n",
        "    for frame_id, content in file_contents.items():\n",
        "        # Create a unique file name for each frame\n",
        "        frame_annotation_file = annotation_file_path / f\"{name}_{frame_id:04d}.txt\"\n",
        "\n",
        "        # Write the content to the file\n",
        "        with open(frame_annotation_file, \"w\") as file:\n",
        "            file.write(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_annotation_batch_train(\n",
        "    name, annotation_file_path, bboxes, image_width, image_height\n",
        "):\n",
        "    \"\"\"\n",
        "    Save annotations in YOLO format for each frame in batches.\n",
        "\n",
        "    Parameters:\n",
        "    - name (str): Base name for the annotation files.\n",
        "    - annotation_file_path (Path): Path object representing the directory to save annotation files.\n",
        "    - bboxes (list of dict): List of bounding boxes, where each bounding box is represented as a dictionary with keys 'frame_id', 'fish_species', 'x', 'y', 'w', 'h'.\n",
        "    - image_width (int): Width of the image.\n",
        "    - image_height (int): Height of the image.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Group bounding boxes by frame_id\n",
        "    frame_bboxes = {}\n",
        "    for bbox in bboxes:\n",
        "        frame_id = bbox[\"frame_id\"]\n",
        "        frame_bboxes.setdefault(frame_id, []).append(bbox)\n",
        "\n",
        "    # Prepare content for each file\n",
        "    file_contents = {}\n",
        "    for frame_id, bboxes in frame_bboxes.items():\n",
        "        # Collect annotation lines for this frame\n",
        "        content = []\n",
        "        for fish in bboxes:\n",
        "            fish_species = fish.get(\"fish_species\", \"\").lower()\n",
        "            x, y, width, height = (\n",
        "                fish.get(\"x\", 0),\n",
        "                fish.get(\"y\", 0),\n",
        "                fish.get(\"w\", 0),\n",
        "                fish.get(\"h\", 0),\n",
        "            )\n",
        "\n",
        "            # Normalize the coordinates\n",
        "            x_center = (x + width / 2.0) / image_width\n",
        "            y_center = (y + height / 2.0) / image_height\n",
        "            width /= image_width\n",
        "            height /= image_height\n",
        "\n",
        "            # Determine the species index\n",
        "            species_index = (\n",
        "                SPECIES_LIST.index(fish_species)\n",
        "                if fish_species in SPECIES_LIST\n",
        "                else UNKNOWN_LABEL\n",
        "            )\n",
        "\n",
        "            # Format the annotation line in YOLO format\n",
        "            content.append(\n",
        "                f\"{species_index} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
        "            )\n",
        "\n",
        "        # Add content for the current frame\n",
        "        file_contents[frame_id] = \"\\n\".join(content)\n",
        "\n",
        "    # Write all files in a batch\n",
        "    for frame_id, content in file_contents.items():\n",
        "        # Create a unique file name for each frame\n",
        "        frame_annotation_file = annotation_file_path / f\"{name}_{frame_id:04d}.txt\"\n",
        "\n",
        "        # Write the content to the file\n",
        "        with open(frame_annotation_file, \"w\") as file:\n",
        "            file.write(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_ground_truth(video_path):\n",
        "    \"\"\"\n",
        "    Extract ground truth annotations from an XML file corresponding to a video.\n",
        "\n",
        "    Parameters:\n",
        "    - video_path (Path): Path object representing the path to the video file.\n",
        "\n",
        "    Returns:\n",
        "    - list of dict: A list of dictionaries, where each dictionary contains the ground truth annotations for a frame.\n",
        "      Each dictionary has the following keys:\n",
        "        - frame_id (int): The ID of the frame.\n",
        "        - fish_species (str): The species of the fish.\n",
        "        - x (int): The x-coordinate of the bounding box.\n",
        "        - y (int): The y-coordinate of the bounding box.\n",
        "        - w (int): The width of the bounding box.\n",
        "        - h (int): The height of the bounding box.\n",
        "    \"\"\"\n",
        "    # Extract the file name without extension from the video path\n",
        "    file_name_without_ext = video_path.stem\n",
        "\n",
        "    # Construct the path to the ground truth XML file\n",
        "    gt_xml_path = GT_DIR / f\"{file_name_without_ext}.xml\"\n",
        "\n",
        "    # Check if the ground truth XML file exists\n",
        "    if not gt_xml_path.exists():\n",
        "        print(f\"Ground truth XML not found: {gt_xml_path}\")\n",
        "        return []\n",
        "\n",
        "    # Parse the XML file\n",
        "    tree = ET.parse(gt_xml_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Initialize an empty list to store ground truth annotations\n",
        "    ground_truth = []\n",
        "\n",
        "    # Iterate over each frame element in the XML\n",
        "    for frame in root.findall(\"frame\"):\n",
        "        frame_id = int(frame.get(\"id\"))\n",
        "\n",
        "        # Iterate over each object element within the frame\n",
        "        for obj in frame.findall(\"object\"):\n",
        "            # Append the ground truth annotation to the list\n",
        "            ground_truth.append(\n",
        "                {\n",
        "                    \"frame_id\": frame_id,\n",
        "                    \"fish_species\": obj.get(\"fish_species\"),\n",
        "                    \"x\": int(obj.get(\"x\")),\n",
        "                    \"y\": int(obj.get(\"y\")),\n",
        "                    \"w\": int(obj.get(\"w\")),\n",
        "                    \"h\": int(obj.get(\"h\")),\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return ground_truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_gmm(frame, frame_idx, gmm_dir, foreground_detector):\n",
        "    \"\"\"\n",
        "    Apply Gaussian Mixture Model (GMM) to a video frame to detect foreground objects and save the results.\n",
        "\n",
        "    Parameters:\n",
        "    - frame (numpy.ndarray): The input video frame.\n",
        "    - frame_idx (int): The index of the current frame.\n",
        "    - gmm_dir (Path): Path object representing the directory to save GMM results.\n",
        "    - foreground_detector (cv2.BackgroundSubtractor): The foreground detector object.\n",
        "\n",
        "    Returns:\n",
        "    - numpy.ndarray: The processed foreground mask.\n",
        "    \"\"\"\n",
        "    # Apply the foreground detector to the frame\n",
        "    foreground = foreground_detector.apply(frame)\n",
        "\n",
        "    # Apply morphological opening to remove noise\n",
        "    filtered_foreground = cv2.morphologyEx(\n",
        "        foreground, cv2.MORPH_OPEN, STRUCTURING_ELEMENT_OPEN\n",
        "    )\n",
        "\n",
        "    # Apply morphological closing to fill gaps\n",
        "    filtered_foreground = cv2.morphologyEx(\n",
        "        filtered_foreground, cv2.MORPH_CLOSE, STRUCTURING_ELEMENT_CLOSE\n",
        "    )\n",
        "\n",
        "    # Find contours in the filtered foreground mask\n",
        "    contours, _ = cv2.findContours(\n",
        "        filtered_foreground, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "    )\n",
        "\n",
        "    # Filter contours based on minimum area and compute bounding boxes\n",
        "    bboxes = [\n",
        "        cv2.boundingRect(c)\n",
        "        for c in contours\n",
        "        if cv2.contourArea(c) >= BLOB_ANALYSIS_PARAMS[\"min_area\"]\n",
        "    ]\n",
        "\n",
        "    # Save the filtered foreground mask as an image\n",
        "    gmm_frame_path = gmm_dir / f\"gmm_img_{frame_idx:04d}.png\"\n",
        "    cv2.imwrite(str(gmm_frame_path), filtered_foreground)\n",
        "\n",
        "    # Save the bounding boxes as annotations\n",
        "    gmm_annotation_path = gmm_dir / f\"gmm_img_{frame_idx:04d}.txt\"\n",
        "    if bboxes:\n",
        "        save_gmm_annotation(\n",
        "            gmm_annotation_path, bboxes, FRAME_RESIZE[0], FRAME_RESIZE[1]\n",
        "        )\n",
        "    else:\n",
        "        # Create an empty annotation file if no bounding boxes are found\n",
        "        gmm_annotation_path.touch()\n",
        "\n",
        "    return filtered_foreground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_optical_flow(frame, frame_idx, prvs, hsv, flow_dir):\n",
        "    \"\"\"\n",
        "    Apply Farneback optical flow to a video frame and save the results.\n",
        "\n",
        "    Parameters:\n",
        "    - frame (numpy.ndarray): The current video frame.\n",
        "    - frame_idx (int): The index of the current frame.\n",
        "    - prvs (numpy.ndarray): The previous grayscale frame.\n",
        "    - hsv (numpy.ndarray): The HSV image used for visualizing the optical flow.\n",
        "    - flow_dir (Path): Path object representing the directory to save optical flow results.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: A tuple containing:\n",
        "        - bgr_resized (numpy.ndarray): The resized BGR image representing the optical flow.\n",
        "        - next_frame (numpy.ndarray): The next grayscale frame.\n",
        "    \"\"\"\n",
        "    # Convert the current frame to grayscale\n",
        "    next_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculate the optical flow using Farneback method\n",
        "    flow = cv2.calcOpticalFlowFarneback(prvs, next_frame, None, **FARNEBACK_PARAMS)\n",
        "\n",
        "    # Compute the magnitude and angle of the flow\n",
        "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "    # Set the hue of the HSV image based on the angle of the flow\n",
        "    hsv[..., 0] = ang * 180 / np.pi / 2\n",
        "\n",
        "    # Set the value of the HSV image based on the normalized magnitude of the flow\n",
        "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "    # Convert the HSV image to BGR for visualization\n",
        "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    # Resize the BGR image to match the desired frame size (640x640)\n",
        "    bgr_resized = cv2.resize(bgr, FRAME_RESIZE)\n",
        "\n",
        "    # Construct the path to save the optical flow frame\n",
        "    flow_frame_path = flow_dir / f\"flow_img_{frame_idx:04d}.png\"\n",
        "\n",
        "    # Save the resized BGR image to the specified directory\n",
        "    cv2.imwrite(str(flow_frame_path), bgr_resized)\n",
        "\n",
        "    return bgr_resized, next_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_combination(\n",
        "    frame, frame_idx, filtered_foreground, bgr_resized, gt_bboxes, combined_dir\n",
        "):\n",
        "    \"\"\"\n",
        "    Combine GMM and Optical Flow images, save the combined image, and save ground truth annotations.\n",
        "\n",
        "    Parameters:\n",
        "    - frame (numpy.ndarray): The original video frame.\n",
        "    - frame_idx (int): The index of the current frame.\n",
        "    - filtered_foreground (numpy.ndarray): The foreground mask obtained from GMM.\n",
        "    - bgr_resized (numpy.ndarray): The resized BGR image obtained from optical flow.\n",
        "    - gt_bboxes (list of dict): List of ground truth bounding boxes for the frame.\n",
        "    - combined_dir (Path): Path object representing the directory to save combined results.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Initialize a blank image with the same shape as the original frame\n",
        "    combined_frame = np.zeros_like(frame)\n",
        "\n",
        "    # Combine the filtered foreground mask into the green channel\n",
        "    combined_frame[:, :, 1] = filtered_foreground\n",
        "\n",
        "    # Combine the blue channel of the resized BGR image into the red channel\n",
        "    combined_frame[:, :, 2] = bgr_resized[:, :, 0]  # Use resized bgr\n",
        "\n",
        "    # Construct the path to save the combined image\n",
        "    combined_frame_path = combined_dir / f\"combined_img_{frame_idx:04d}.png\"\n",
        "\n",
        "    # Save the combined image to the specified directory\n",
        "    cv2.imwrite(str(combined_frame_path), combined_frame)\n",
        "\n",
        "    # Construct the path to save the ground truth annotations\n",
        "    combined_annotation_path = combined_dir / f\"combined_img_{frame_idx:04d}.txt\"\n",
        "    name = \"combined_img\"\n",
        "\n",
        "    # Save the ground truth annotations if they exist\n",
        "    if gt_bboxes:\n",
        "        if \"train\" in str(combined_dir):\n",
        "            save_annotation_batch_train(\n",
        "                name,\n",
        "                combined_dir,\n",
        "                gt_bboxes,\n",
        "                FRAME_RESIZE[0],\n",
        "                FRAME_RESIZE[1],\n",
        "            )\n",
        "        \n",
        "        if \"test\" in str(combined_dir):\n",
        "            save_annotation_batch_test(\n",
        "                name,\n",
        "                combined_dir,\n",
        "                gt_bboxes,\n",
        "                FRAME_RESIZE[0],\n",
        "                FRAME_RESIZE[1],\n",
        "            )\n",
        "\n",
        "    else:\n",
        "        # Create an empty annotation file if no ground truth bounding boxes are found\n",
        "        combined_annotation_path.touch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_frame(\n",
        "    frame,\n",
        "    frame1,\n",
        "    frame_idx,\n",
        "    gt_bboxes,\n",
        "    foreground_detector,\n",
        "    prvs,\n",
        "    hsv,\n",
        "    img_dir,\n",
        "    gmm_dir,\n",
        "    flow_dir,\n",
        "    combined_dir,\n",
        "):\n",
        "    \"\"\"\n",
        "    Process a video frame by applying GMM and Optical Flow, and save the results.\n",
        "\n",
        "    Parameters:\n",
        "    - frame (numpy.ndarray): The current video frame.\n",
        "    - frame1 (numpy.ndarray): The next video frame for optical flow calculation.\n",
        "    - frame_idx (int): The index of the current frame.\n",
        "    - gt_bboxes (list of dict): List of ground truth bounding boxes for the frame.\n",
        "    - foreground_detector (cv2.BackgroundSubtractor): The foreground detector object.\n",
        "    - prvs (numpy.ndarray): The previous grayscale frame for optical flow calculation.\n",
        "    - hsv (numpy.ndarray): The HSV image used for visualizing the optical flow.\n",
        "    - img_dir (Path): Path object representing the directory to save original frames.\n",
        "    - gmm_dir (Path): Path object representing the directory to save GMM results.\n",
        "    - flow_dir (Path): Path object representing the directory to save optical flow results.\n",
        "    - combined_dir (Path): Path object representing the directory to save combined results.\n",
        "\n",
        "    Returns:\n",
        "    - numpy.ndarray: The next grayscale frame for optical flow calculation.\n",
        "    \"\"\"\n",
        "    # Save the original frame to the img_dir\n",
        "    img_frame_path = img_dir / f\"img_{frame_idx:04d}.png\"\n",
        "    cv2.imwrite(str(img_frame_path), frame)\n",
        "\n",
        "    # Save annotations for the original frame (train_img)\n",
        "    img_annotation_path = img_dir / f\"img_{frame_idx:04d}.txt\"\n",
        "    name = \"img\"\n",
        "    if gt_bboxes:\n",
        "        # Save ground truth annotations if they exist\n",
        "        if \"train\" in str(img_dir):\n",
        "            save_annotation_batch_train(\n",
        "                name,\n",
        "                img_dir,\n",
        "                gt_bboxes,\n",
        "                FRAME_RESIZE[0],\n",
        "                FRAME_RESIZE[1],\n",
        "            )\n",
        "        \n",
        "        if \"test\" in str(img_dir):\n",
        "            save_annotation_batch_test(\n",
        "                name,\n",
        "                img_dir,\n",
        "                gt_bboxes,\n",
        "                FRAME_RESIZE[0],\n",
        "                FRAME_RESIZE[1],\n",
        "            )\n",
        "    else:\n",
        "        # Create an empty annotation file if no ground truth bounding boxes are found\n",
        "        img_annotation_path.touch()\n",
        "\n",
        "    # Apply GMM to the frame to detect foreground objects\n",
        "    foreground = apply_gmm(frame, frame_idx, gmm_dir, foreground_detector)\n",
        "\n",
        "    # Apply optical flow to the next frame\n",
        "    bgr, next_frame = apply_optical_flow(frame1, frame_idx, prvs, hsv, flow_dir)\n",
        "\n",
        "    # Combine GMM and optical flow results and save the combined image\n",
        "    apply_combination(frame, frame_idx, foreground, bgr, gt_bboxes, combined_dir)\n",
        "\n",
        "    return next_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_video(video_path):\n",
        "    \"\"\"\n",
        "    Process a video to extract ground truth, apply GMM and optical flow, and save the results.\n",
        "\n",
        "    Parameters:\n",
        "    - video_path (Path): Path object representing the path to the video file.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Extract the last 15 characters of the video file name (without extension) to use as a directory name\n",
        "    video_name_short = video_path.stem[-15:]\n",
        "\n",
        "    # Define directories for saving images, GMM results, optical flow results, and combined results\n",
        "    img_dir = IMG_DIR / video_name_short\n",
        "    gmm_dir = GMM_DIR / video_name_short\n",
        "    flow_dir = OPTICAL_DIR / video_name_short\n",
        "    combined_dir = GMM_OPTICAL_DIR / video_name_short\n",
        "\n",
        "    # Create the directories if they do not exist\n",
        "    for directory in [img_dir, gmm_dir, flow_dir, combined_dir]:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Extract ground truth bounding boxes from the corresponding XML file\n",
        "    gt_bboxes = extract_ground_truth(video_path)\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "\n",
        "    # Get the total number of frames in the video\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Create a foreground detector using MOG2\n",
        "    foreground_detector = cv2.createBackgroundSubtractorMOG2(\n",
        "        **FOREGROUND_DETECTOR_PARAMS\n",
        "    )\n",
        "\n",
        "    # Read the first frame of the video\n",
        "    ret, frame1 = cap.read()\n",
        "\n",
        "    # Check if the video file was read successfully\n",
        "    if not ret:\n",
        "        print(f\"Failed to read the video file: {video_path}\")\n",
        "        return\n",
        "\n",
        "    # Convert the first frame to grayscale for optical flow calculation\n",
        "    prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Initialize an HSV image for visualizing optical flow\n",
        "    hsv = np.zeros_like(frame1)\n",
        "    hsv[..., 1] = 255\n",
        "\n",
        "    # Initialize the frame index\n",
        "    frame_idx = 0\n",
        "\n",
        "    # Process each frame of the video\n",
        "    with tqdm(total=total_frames, desc=f\"Processing {video_name_short}\") as video_pbar:\n",
        "        while ret:\n",
        "            # Resize the frame and adjust its gamma\n",
        "            frame = cv2.resize(frame1, FRAME_RESIZE)\n",
        "            frame = adjust_gamma(frame, FRAME_ADJUST_GAMMA)\n",
        "\n",
        "            # Process the current frame\n",
        "            next_frame = process_frame(\n",
        "                frame,\n",
        "                frame1,\n",
        "                frame_idx,\n",
        "                gt_bboxes,\n",
        "                foreground_detector,\n",
        "                prvs,\n",
        "                hsv,\n",
        "                img_dir,\n",
        "                gmm_dir,\n",
        "                flow_dir,\n",
        "                combined_dir,\n",
        "            )\n",
        "\n",
        "            # Update the progress bar\n",
        "            video_pbar.update(1)\n",
        "\n",
        "            # Update the previous frame for optical flow calculation\n",
        "            prvs = next_frame\n",
        "\n",
        "            # Read the next frame of the video\n",
        "            ret, frame1 = cap.read()\n",
        "\n",
        "            # Increment the frame index\n",
        "            frame_idx += 1\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing #201108091140_6:   0%|          | 0/300 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "argument of type 'PosixPath' is not iterable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[56], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Iterate over each video file and process it\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m video_files:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[51], line 64\u001b[0m, in \u001b[0;36mprocess_video\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m frame \u001b[38;5;241m=\u001b[39m adjust_gamma(frame, FRAME_ADJUST_GAMMA)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Process the current frame\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m next_frame \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeground_detector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhsv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgmm_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflow_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombined_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Update the progress bar\u001b[39;00m\n\u001b[1;32m     79\u001b[0m video_pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
            "Cell \u001b[0;32mIn[50], line 42\u001b[0m, in \u001b[0;36mprocess_frame\u001b[0;34m(frame, frame1, frame_idx, gt_bboxes, foreground_detector, prvs, hsv, img_dir, gmm_dir, flow_dir, combined_dir)\u001b[0m\n\u001b[1;32m     39\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gt_bboxes:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Save ground truth annotations if they exist\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg_dir\u001b[49m:\n\u001b[1;32m     43\u001b[0m         save_annotation_batch_train(\n\u001b[1;32m     44\u001b[0m             name,\n\u001b[1;32m     45\u001b[0m             img_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m             FRAME_RESIZE[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     49\u001b[0m         )\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m img_dir:\n",
            "\u001b[0;31mTypeError\u001b[0m: argument of type 'PosixPath' is not iterable"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Main function to process all video files in the specified directory.\n",
        "\n",
        "This function searches for video files with .flv and .avi extensions in the VIDEO_DIR,\n",
        "and processes each video file using the process_video function.\n",
        "\"\"\"\n",
        "\n",
        "# Do not run this, if image data is already there\n",
        "\n",
        "# Get a list of all .flv and .avi video files in the VIDEO_DIR\n",
        "video_files = list(VIDEO_DIR.glob(\"*.flv\")) + list(VIDEO_DIR.glob(\"*.avi\"))\n",
        "\n",
        "# Iterate over each video file and process it\n",
        "for video in video_files:\n",
        "    process_video(video)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n# Get a list of all .flv and .avi video files in the VIDEO_DIR\\nvideo_files = list(VIDEO_DIR.glob(\"*.flv\")) + list(VIDEO_DIR.glob(\"*.avi\"))\\n\\n# Use ThreadPoolExecutor to process videos concurrently\\nwith ThreadPoolExecutor() as executor:\\n    # Submit all video processing tasks to the thread pool\\n    futures = [executor.submit(process_video, video) for video in video_files]\\n\\n    # Optionally, wait for all the futures to complete and handle any exceptions\\n    for future in as_completed(futures):\\n        try:\\n            future.result()  # Retrieve the result of the function (if any)\\n        except Exception as exc:\\n            print(f\"An error occurred: {exc}\")\\n'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Main function to process all video files in the specified directory.\n",
        "\n",
        "This function searches for video files with .flv and .avi extensions in the VIDEO_DIR,\n",
        "and processes each video file using the process_video function.\n",
        "\"\"\"\n",
        "# Do not run this, if image data is already there\n",
        "\"\"\"\n",
        "# Get a list of all .flv and .avi video files in the VIDEO_DIR\n",
        "video_files = list(VIDEO_DIR.glob(\"*.flv\")) + list(VIDEO_DIR.glob(\"*.avi\"))\n",
        "\n",
        "# Use ThreadPoolExecutor to process videos concurrently\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    # Submit all video processing tasks to the thread pool\n",
        "    futures = [executor.submit(process_video, video) for video in video_files]\n",
        "\n",
        "    # Optionally, wait for all the futures to complete and handle any exceptions\n",
        "    for future in as_completed(futures):\n",
        "        try:\n",
        "            future.result()  # Retrieve the result of the function (if any)\n",
        "        except Exception as exc:\n",
        "            print(f\"An error occurred: {exc}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw6y7yDLPvRZ"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZupVk2dN8pX"
      },
      "source": [
        "## Create train.txt for YOLO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "OQf3YXnPawwr"
      },
      "outputs": [],
      "source": [
        "output_file = BASE_DIR / \"test.txt\"  # Name of the output file\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    # Walk through all folders and subfolders\n",
        "    for dirpath, _, filenames in os.walk(IMG_DIR):\n",
        "        for filename in filenames:\n",
        "            if filename.lower().endswith(\".png\"):\n",
        "                full_path = os.path.join(dirpath, filename)\n",
        "                f.write(full_path + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8yDT_9maBAh"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New https://pypi.org/project/ultralytics/8.2.82 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.2.80 ðŸš€ Python-3.10.1 torch-2.4.0 CPU (Apple M1)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=config.yaml, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/Users/jan/Documents/code/cv/project/runs/detect/train4\n",
            "Overriding model.yaml nc=80 with nc=16\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    754432  ultralytics.nn.modules.head.Detect           [16, [64, 128, 256]]          \n",
            "YOLOv8n summary: 225 layers, 3,013,968 parameters, 3,013,952 gradients, 8.2 GFLOPs\n",
            "\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages/ultralytics/engine/trainer.py:271: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/jan/Documents/code/cv/project/train_img/#201011150930_0... 5278 images, 3726 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9004/9004 [00:01<00:00, 4810.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /Users/jan/Documents/code/cv/project/train_img/#201106090830_0/img_0074.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /Users/jan/Documents/code/cv/project/train_img/#201108061200_0/img_0520.jpg: 1 duplicate labels removed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/jan/Documents/code/cv/project/train_img/#201011150930_0.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/jan/Documents/code/cv/project/test_img/#201103060650_1... 10087 images, 12261 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21748/21748 [00:04<00:00, 5349.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/jan/Documents/code/cv/project/test_img/#201104211310_7/img_0173.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/jan/Documents/code/cv/project/test_img/#201108220720_4/img_0095.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/jan/Documents/code/cv/project/test_img/#201109201740_0/img_0006.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/jan/Documents/code/cv/project/test_img/#201109241310_1/img_0200.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/jan/Documents/code/cv/project/test_img/#201109241310_1/img_0201.jpg: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/jan/Documents/code/cv/project/test_img/#201110151400_6/img_0044.jpg: 1 duplicate labels removed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/jan/Documents/code/cv/project/test_img/#201103060650_1.cache\n",
            "\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0005, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/Users/jan/Documents/code/cv/project/runs/detect/train4\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/1         0G      5.053         14      4.419         40        640:   0%|          | 2/563 [00:15<1:10:51,  7.58s/it]"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
        "\n",
        "\n",
        "# Use the model\n",
        "result = model.train(data=\"config.yaml\", epochs=1)  # train the model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SPN4GsCpZC4W",
        "BOgY7NnkQO4S",
        "bB9eG7qbV6jl",
        "hfFX_FAOYTQt",
        "xdYij-suZLWQ",
        "xPNG9tgHZa18",
        "3NNPM9OqZoGw",
        "2GFUTgDpZwlJ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
