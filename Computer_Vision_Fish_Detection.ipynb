{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPN4GsCpZC4W"
      },
      "source": [
        "# Preliminary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKNTf7L4b2z7"
      },
      "source": [
        "Here we install all imports and other necessary components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1H3gELF5H9o"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccCEcC7sI89U",
        "outputId": "69ccc84a-99bf-439e-fe3d-3cf05d39cd9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov7'...\n",
            "remote: Enumerating objects: 1197, done.\u001b[K\n",
            "remote: Total 1197 (delta 0), reused 0 (delta 0), pack-reused 1197 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1197/1197), 74.23 MiB | 18.77 MiB/s, done.\n",
            "Resolving deltas: 100% (519/519), done.\n",
            "/Users/jan/Documents/code/cv/project/yolov7/yolov7\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (3.9.1.post1)\n",
            "Requirement already satisfied: numpy<1.24.0,>=1.18.5 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (10.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (1.14.0)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.7.0 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (2.4.0)\n",
            "Requirement already satisfied: torchvision!=0.13.0,>=0.8.1 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.19.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (4.66.5)\n",
            "Requirement already satisfied: protobuf<4.21.3 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (4.21.2)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (2.17.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (0.13.2)\n",
            "Requirement already satisfied: ipython in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 34)) (8.12.3)\n",
            "Requirement already satisfied: psutil in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 35)) (6.0.0)\n",
            "Requirement already satisfied: thop in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from -r requirements.txt (line 36)) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2024.7.4)\n",
            "Requirement already satisfied: filelock in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (1.13.2)\n",
            "Requirement already satisfied: networkx in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (2024.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.66.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.7)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (58.1.0)\n",
            "Requirement already satisfied: six>1.9 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from pandas>=1.1.4->-r requirements.txt (line 21)) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from pandas>=1.1.4->-r requirements.txt (line 21)) (2024.1)\n",
            "Requirement already satisfied: backcall in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 34)) (0.2.0)\n",
            "Requirement already satisfied: decorator in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 34)) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 34)) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 34)) (0.1.7)\n",
            "Requirement already satisfied: pickleshare in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 34)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 34)) (3.0.47)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 34)) (2.18.0)\n",
            "Requirement already satisfied: stack-data in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 34)) (0.6.3)\n",
            "Requirement already satisfied: traitlets>=5 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 34)) (5.14.3)\n",
            "Requirement already satisfied: pexpect>4.3 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 34)) (4.9.0)\n",
            "Requirement already satisfied: appnope in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from ipython->-r requirements.txt (line 34)) (0.1.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from jedi>=0.16->ipython->-r requirements.txt (line 34)) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from pexpect>4.3->ipython->-r requirements.txt (line 34)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->-r requirements.txt (line 34)) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->-r requirements.txt (line 17)) (2.1.5)\n",
            "Requirement already satisfied: executing>=1.2.0 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from stack-data->ipython->-r requirements.txt (line 34)) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from stack-data->ipython->-r requirements.txt (line 34)) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from stack-data->ipython->-r requirements.txt (line 34)) (0.2.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jan/Documents/code/cv/project/cv_project/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install imageio\n",
        "!pip install numpy\n",
        "!pip install opencv-python\n",
        "!pip install tqdm\n",
        "!pip install patool\n",
        "!git clone https://github.com/WongKinYiu/yolov7\n",
        "%cd yolov7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE2jk4zOmoPu",
        "outputId": "a9c5728a-4ed8-42a8-f8d6-a60268f21504"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import patoolib\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "from IPython.display import display, Image\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from psutil import virtual_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MoiM3ryXiBG",
        "outputId": "b65b2bc9-d6c6-4d71-ebad-17cbdfd6177f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: nvidia-smi\n",
            "Your runtime has 17.2 gigabytes of available RAM\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check what GPU is available\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# Check how much RAM is available\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Mn3ayM5PUR"
      },
      "source": [
        "## Directories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "RafB5uuaOSMK"
      },
      "outputs": [],
      "source": [
        "# Configuration Flags\n",
        "CREATE = False # Flag to enable image pipeline\n",
        "SAVE_ORIGINAL = False  # Flag to save original frames\n",
        "RELEASE = False  # Flag to switch between concurrent and sequential processing\n",
        "\n",
        "# Base directory setup\n",
        "BASE_DIR = Path(\"/Users/jan/Documents/code/cv/project\")\n",
        "\n",
        "# Training set directories\n",
        "TRAIN_VIDEO_DIR = BASE_DIR / \"data/fishclef_2015_release/training_set/videos\"\n",
        "TRAIN_GT_DIR = BASE_DIR / \"data/fishclef_2015_release/training_set/gt\"\n",
        "TRAIN_IMG_DIR = BASE_DIR / \"train_img/\"\n",
        "TRAIN_GMM_DIR = BASE_DIR / \"train_gmm/\"\n",
        "TRAIN_OPTICAL_DIR = BASE_DIR / \"train_optical/\"\n",
        "TRAIN_GMM_OPTICAL_DIR = BASE_DIR / \"train_gmm_optical/\"\n",
        "\n",
        "# Test set directories\n",
        "TEST_VIDEO_DIR = BASE_DIR / \"data/fishclef_2015_release/test_set/videos\"\n",
        "TEST_GT_DIR = BASE_DIR / \"data/fishclef_2015_release/test_set/gt\"\n",
        "TEST_IMG_DIR = BASE_DIR / \"test_img/\"\n",
        "TEST_GMM_DIR = BASE_DIR / \"test_gmm/\"\n",
        "TEST_OPTICAL_DIR = BASE_DIR / \"test_optical/\"\n",
        "TEST_GMM_OPTICAL_DIR = BASE_DIR / \"test_gmm_optical/\"\n",
        "\n",
        "# List of species names\n",
        "SPECIES_LIST = [\n",
        "    \"abudefduf vaigiensis\",\n",
        "    \"acanthurus nigrofuscus\",\n",
        "    \"amphiprion clarkii\",\n",
        "    \"chaetodon lununatus\",\n",
        "    \"chaetodon speculum\",\n",
        "    \"chaetodon trifascialis\",\n",
        "    \"chromis chrysura\",\n",
        "    \"dascyllus aruanus\",\n",
        "    \"dascyllus reticulatus\",\n",
        "    \"hemigumnus malapterus\",\n",
        "    \"myripristis kuntee\",\n",
        "    \"neoglyphidodon nigroris\",\n",
        "    \"pempheris vanicolensis\",\n",
        "    \"plectrogly-phidodon dickii\",\n",
        "    \"zebrasoma scopas\",\n",
        "]\n",
        "\n",
        "# Label for unknown species\n",
        "UNKNOWN_LABEL = 15\n",
        "\n",
        "# Frame processing parameters\n",
        "FRAME_RESIZE = (240, 240)\n",
        "\n",
        "# Optical flow parameters\n",
        "FARNEBACK_PARAMS = {\n",
        "    \"pyr_scale\": 0.5,\n",
        "    \"levels\": 3,\n",
        "    \"winsize\": 15,\n",
        "    \"iterations\": 3,\n",
        "    \"poly_n\": 5,\n",
        "    \"poly_sigma\": 1.2,\n",
        "    \"flags\": 0,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMR9UH9jPyNX"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ymY_V2japXC"
      },
      "source": [
        "# Create training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "pMM9grDh0b5q"
      },
      "outputs": [],
      "source": [
        "def adjust_gamma(image, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Adjusts the gamma of an image.\n",
        "\n",
        "    Args:\n",
        "        image (np.ndarray): Input image.\n",
        "        gamma (float): Gamma value to adjust (default is 1.0).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Gamma adjusted image.\n",
        "    \"\"\"\n",
        "    invGamma = 1.0 / gamma\n",
        "    table = np.array([(i / 255.0) ** invGamma * 255 for i in range(256)], dtype=\"uint8\")\n",
        "    return cv2.LUT(image, table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_annotation(\n",
        "    name,\n",
        "    annotation_file_path,\n",
        "    bboxes,\n",
        "    image_width,\n",
        "    image_height,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates YOLO format annotations for bounding boxes and saves them to files.\n",
        "\n",
        "    Args:\n",
        "        name (str): Name prefix for saved annotation files.\n",
        "        annotation_file_path (Path): Path where annotation files will be saved.\n",
        "        bboxes (list): List of bounding boxes for the frame.\n",
        "        image_width (int): Width of the image.\n",
        "        image_height (int): Height of the image.\n",
        "        species_key (str): Key for accessing species name in bbox dictionary (default is 'fish_species').\n",
        "    \"\"\"\n",
        "    frame_bboxes = {}\n",
        "    \n",
        "    for bbox in bboxes:\n",
        "        frame_id = bbox[\"frame_id\"]\n",
        "        frame_bboxes.setdefault(frame_id, []).append(bbox)\n",
        "\n",
        "    for frame_id, bboxes in frame_bboxes.items():\n",
        "        annotations = []\n",
        "        for fish in bboxes:\n",
        "            fish_species = fish.get(\"fish_species\", \"\").lower()\n",
        "            x, y, width, height = (\n",
        "                fish.get(\"x\", 0),\n",
        "                fish.get(\"y\", 0),\n",
        "                fish.get(\"w\", 0),\n",
        "                fish.get(\"h\", 0),\n",
        "            )\n",
        "            x_center = (x + width / 2.0) / image_width\n",
        "            y_center = (y + height / 2.0) / image_height\n",
        "            width /= image_width\n",
        "            height /= image_height\n",
        "            species_index = (\n",
        "                SPECIES_LIST.index(fish_species)\n",
        "                if fish_species in SPECIES_LIST\n",
        "                else UNKNOWN_LABEL\n",
        "            )\n",
        "            annotations.append(\n",
        "                f\"{species_index} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
        "            )\n",
        "\n",
        "        frame_annotation_file = annotation_file_path / f\"{name}_{frame_id:04d}.txt\"\n",
        "        with open(frame_annotation_file, \"w\") as file:\n",
        "            file.write(\"\\n\".join(annotations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_ground_truth(video_path, species_key):\n",
        "    \"\"\"\n",
        "    Extracts ground truth annotations from the corresponding XML file.\n",
        "\n",
        "    Args:\n",
        "        video_path (Path): Path to the video file.\n",
        "\n",
        "    Returns:\n",
        "        list: List of ground truth bounding boxes extracted from XML.\n",
        "    \"\"\"\n",
        "    file_name_without_ext = video_path.stem\n",
        "    gt_xml_path = TEST_GT_DIR / f\"{file_name_without_ext}.xml\"\n",
        "\n",
        "    if not gt_xml_path.exists():\n",
        "        print(f\"Ground truth XML not found: {gt_xml_path}\")\n",
        "        return []\n",
        "\n",
        "    tree = ET.parse(gt_xml_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    ground_truth = []\n",
        "    for frame in root.findall(\"frame\"):\n",
        "        frame_id = int(frame.get(\"id\"))\n",
        "        for obj in frame.findall(\"object\"):\n",
        "            ground_truth.append(\n",
        "                {\n",
        "                    \"frame_id\": frame_id,\n",
        "                    \"fish_species\": obj.get(species_key),\n",
        "                    \"x\": int(obj.get(\"x\")),\n",
        "                    \"y\": int(obj.get(\"y\")),\n",
        "                    \"w\": int(obj.get(\"w\")),\n",
        "                    \"h\": int(obj.get(\"h\")),\n",
        "                }\n",
        "            )\n",
        "    \n",
        "    return ground_truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_gmm(frame, foreground_detector):\n",
        "    \"\"\"\n",
        "    Applies GMM (Gaussian Mixture Model) to detect foreground objects in a frame.\n",
        "\n",
        "    Args:\n",
        "        frame (np.ndarray): Input frame.\n",
        "        foreground_detector (cv2.BackgroundSubtractorMOG2): Foreground detector.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Filtered foreground mask.\n",
        "    \"\"\"\n",
        "    # frame_denoised = cv2.fastNlMeansDenoising(frame, None)\n",
        "    foreground = foreground_detector.apply(frame)\n",
        "    filtered_foreground = cv2.morphologyEx(\n",
        "        foreground, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    )\n",
        "    filtered_foreground = cv2.morphologyEx(\n",
        "        filtered_foreground,\n",
        "        cv2.MORPH_CLOSE,\n",
        "        cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)),\n",
        "    )\n",
        "    # frame_denoised = cv2.fastNlMeansDenoising(filtered_foreground, None)\n",
        "    \n",
        "    # Shadow Removal: Convert shadows to binary foreground\n",
        "    _, filtered_foreground = cv2.threshold(\n",
        "        filtered_foreground, 127, 255, cv2.THRESH_BINARY\n",
        "    )\n",
        "    \n",
        "    return filtered_foreground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_optical_flow(frame, prvs, hsv):\n",
        "    \"\"\"\n",
        "    Computes optical flow using Farneback method and visualizes it in HSV space.\n",
        "\n",
        "    Args:\n",
        "        frame (np.ndarray): Input frame.\n",
        "        prvs (np.ndarray): Previous frame in grayscale.\n",
        "        hsv (np.ndarray): HSV image used for optical flow visualization.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple containing resized BGR image of the flow and next grayscale frame.\n",
        "    \"\"\"\n",
        "    next_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    flow = cv2.calcOpticalFlowFarneback(prvs, next_frame, None, **FARNEBACK_PARAMS)\n",
        "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "    hsv[..., 0] = ang * 180 / np.pi / 2\n",
        "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "    bgr_resized = cv2.resize(bgr, FRAME_RESIZE)\n",
        "    \n",
        "    return bgr_resized, next_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_combination(\n",
        "    frame, frame_idx, filtered_foreground, bgr_resized, gt_bboxes, combined_dir\n",
        "):\n",
        "    \"\"\"\n",
        "    Combines the results of GMM and optical flow, and saves the combined image and annotations.\n",
        "\n",
        "    Args:\n",
        "        frame (np.ndarray): Original frame.\n",
        "        frame_idx (int): Frame index.\n",
        "        filtered_foreground (np.ndarray): Foreground mask obtained from GMM.\n",
        "        bgr_resized (np.ndarray): Optical flow visualization in BGR format.\n",
        "        gt_bboxes (list): List of ground truth bounding boxes.\n",
        "        combined_dir (Path): Directory to save the combined image and annotations.\n",
        "    \"\"\"\n",
        "    combined_frame = np.zeros_like(frame)\n",
        "    combined_frame[:, :, 0] = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    combined_frame[:, :, 1] = filtered_foreground\n",
        "    combined_frame[:, :, 2] = bgr_resized[:, :, 0]\n",
        "    combined_frame_path = combined_dir / f\"combined_img_{frame_idx:04d}.jpg\"\n",
        "    cv2.imwrite(str(combined_frame_path), combined_frame)\n",
        "\n",
        "    if gt_bboxes:\n",
        "        get_annotation(\n",
        "            \"combined_img\",\n",
        "            combined_dir,\n",
        "            gt_bboxes,\n",
        "            FRAME_RESIZE[0],\n",
        "            FRAME_RESIZE[1],\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_frame(\n",
        "    frame,\n",
        "    frame1,\n",
        "    frame_idx,\n",
        "    gt_bboxes,\n",
        "    foreground_detector,\n",
        "    prvs,\n",
        "    hsv,\n",
        "    img_dir,\n",
        "    combined_dir,\n",
        "):\n",
        "    \"\"\"\n",
        "    Processes a single video frame by applying background subtraction (GMM) and optical flow,\n",
        "    and then combines the results. Optionally saves the original frame, and stores the combined\n",
        "    output along with ground truth annotations.\n",
        "\n",
        "    This function performs the following steps for a given frame:\n",
        "\n",
        "    1. Optionally saves the original frame to a specified directory.\n",
        "    2. Applies Gaussian Mixture Model (GMM) to detect foreground objects in the frame.\n",
        "    3. Computes optical flow between the current and next frame to track movement.\n",
        "    4. Combines the GMM results and optical flow into a final output image.\n",
        "    5. Saves the combined image and associated ground truth annotations to the specified directory.\n",
        "\n",
        "    Args:\n",
        "        frame (numpy.ndarray): The current video frame after resizing and gamma adjustment.\n",
        "        frame1 (numpy.ndarray): The next video frame to compute optical flow.\n",
        "        frame_idx (int): The index of the current frame in the video.\n",
        "        gt_bboxes (list): Ground truth bounding boxes for objects (fish) in the frame.\n",
        "        foreground_detector (cv2.BackgroundSubtractor): Foreground detector based on GMM.\n",
        "        prvs (numpy.ndarray): The previous grayscale frame used for optical flow calculation.\n",
        "        hsv (numpy.ndarray): The HSV image used for visualizing optical flow.\n",
        "        img_dir (Path): Directory to save the original frames.\n",
        "        combined_dir (Path): Directory to save the combined results of GMM and optical flow.\n",
        "\n",
        "    Returns:\n",
        "        next_frame (numpy.ndarray): The grayscale version of the current frame (frame1) for use in the next iteration of optical flow calculation.\n",
        "    \"\"\"\n",
        "    if SAVE_ORIGINAL:\n",
        "        # Save the original frame to the img_dir\n",
        "        img_frame_path = img_dir / f\"img_{frame_idx:04d}.png\"\n",
        "        cv2.imwrite(str(img_frame_path), frame)\n",
        "\n",
        "    # Apply GMM to the frame to detect foreground objects\n",
        "    foreground = apply_gmm(frame, foreground_detector)\n",
        "\n",
        "    # Apply optical flow to the next frame\n",
        "    bgr, next_frame = apply_optical_flow(frame1, prvs, hsv)\n",
        "\n",
        "    # Combine GMM and optical flow results and save the combined image\n",
        "    apply_combination(frame, frame_idx, foreground, bgr, gt_bboxes, combined_dir)\n",
        "\n",
        "    return next_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_video(video_path):\n",
        "    \"\"\"\n",
        "    Processes a video by applying background subtraction (using Gaussian Mixture Model),\n",
        "    optical flow, and frame adjustments, and saves the processed frames and combined results\n",
        "    along with ground truth annotations.\n",
        "\n",
        "    This function extracts frames from the input video, performs foreground detection using\n",
        "    a Gaussian Mixture Model (GMM), calculates optical flow for movement detection, and\n",
        "    combines these results. The processed frames and combined images are saved in specific\n",
        "    directories. Additionally, it uses ground truth bounding boxes extracted from an\n",
        "    XML file for annotation purposes.\n",
        "\n",
        "    Args:\n",
        "        video_path (Path): Path to the input video file.\n",
        "\n",
        "    Steps:\n",
        "        1. Extract ground truth bounding boxes for the video from the corresponding XML file.\n",
        "        2. Create directories to store processed images and combined results.\n",
        "        3. Open the video and initialize background subtraction (GMM) and optical flow.\n",
        "        4. Process each frame in the video:\n",
        "            - Resize and adjust gamma for frame.\n",
        "            - Apply background subtraction (GMM) for foreground detection.\n",
        "            - Compute optical flow to detect movement.\n",
        "            - Combine the results of GMM and optical flow.\n",
        "            - Save the processed frames and results.\n",
        "        5. Release the video capture object when done.\n",
        "\n",
        "    The function also includes progress tracking using tqdm to visualize the video processing progress.\n",
        "\n",
        "    Parameters:\n",
        "        video_path (Path): Path to the video file being processed.\n",
        "\n",
        "    Returns:\n",
        "        None: The function processes the video, saves results, and does not return anything.\n",
        "    \"\"\"\n",
        "\n",
        "    video_name_short = video_path.stem[-15:]\n",
        "    img_dir = TEST_IMG_DIR / video_name_short\n",
        "    combined_dir = TEST_GMM_OPTICAL_DIR / video_name_short\n",
        "\n",
        "    for directory in [combined_dir]:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "    \n",
        "    if SAVE_ORIGINAL:\n",
        "        for directory in [img_dir]:\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Consider different GT names\n",
        "    species_key = \"\"\n",
        "    if \"train\" in str(combined_dir):\n",
        "        species_key = \"fish_species\"\n",
        "    if \"test\" in str(combined_dir):\n",
        "        species_key = \"species_name\"\n",
        "    \n",
        "    # Extract ground truth bounding boxes from the corresponding XML file\n",
        "    gt_bboxes = extract_ground_truth(video_path, species_key)\n",
        "\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    foreground_detector = cv2.createBackgroundSubtractorMOG2(\n",
        "        history=250, varThreshold=16, detectShadows=True\n",
        "    )\n",
        "\n",
        "    ret, frame1 = cap.read()\n",
        "    if not ret:\n",
        "        print(f\"Failed to read the video file: {video_path}\")\n",
        "        return\n",
        "\n",
        "    prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "    hsv = np.zeros_like(frame1)\n",
        "    hsv[..., 1] = 255\n",
        "    frame_idx = 0\n",
        "\n",
        "    # Process each frame of the video\n",
        "    with tqdm(total=total_frames, desc=f\"Processing {video_name_short}\") as video_pbar:\n",
        "        while ret:\n",
        "            frame = cv2.resize(frame1, FRAME_RESIZE)\n",
        "            frame = adjust_gamma(frame, 1.5)\n",
        "            frame_blurred = cv2.GaussianBlur(frame, (5, 5), 0)\n",
        "\n",
        "            # Process the current frame\n",
        "            next_frame = process_frame(\n",
        "                frame_blurred,\n",
        "                frame1,\n",
        "                frame_idx,\n",
        "                gt_bboxes,\n",
        "                foreground_detector,\n",
        "                prvs,\n",
        "                hsv,\n",
        "                img_dir,\n",
        "                combined_dir,\n",
        "            )\n",
        "\n",
        "            video_pbar.update(1)\n",
        "            prvs = next_frame\n",
        "            ret, frame1 = cap.read()\n",
        "            frame_idx += 1\n",
        "\n",
        "    cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing #201108091140_6:   0%|          | 0/300 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "argument of type 'PosixPath' is not iterable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[56], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Iterate over each video file and process it\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video \u001b[38;5;129;01min\u001b[39;00m video_files:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[51], line 64\u001b[0m, in \u001b[0;36mprocess_video\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m frame \u001b[38;5;241m=\u001b[39m adjust_gamma(frame, FRAME_ADJUST_GAMMA)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Process the current frame\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m next_frame \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_bboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeground_detector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhsv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgmm_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflow_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcombined_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Update the progress bar\u001b[39;00m\n\u001b[1;32m     79\u001b[0m video_pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
            "Cell \u001b[0;32mIn[50], line 42\u001b[0m, in \u001b[0;36mprocess_frame\u001b[0;34m(frame, frame1, frame_idx, gt_bboxes, foreground_detector, prvs, hsv, img_dir, gmm_dir, flow_dir, combined_dir)\u001b[0m\n\u001b[1;32m     39\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gt_bboxes:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Save ground truth annotations if they exist\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg_dir\u001b[49m:\n\u001b[1;32m     43\u001b[0m         save_annotation_batch_train(\n\u001b[1;32m     44\u001b[0m             name,\n\u001b[1;32m     45\u001b[0m             img_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m             FRAME_RESIZE[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     49\u001b[0m         )\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m img_dir:\n",
            "\u001b[0;31mTypeError\u001b[0m: argument of type 'PosixPath' is not iterable"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Main entry point of the script. Processes either training or test videos.\n",
        "\"\"\"\n",
        "video_files = list(TEST_VIDEO_DIR.glob(\"*.flv\")) + list(\n",
        "    TEST_VIDEO_DIR.glob(\"*.avi\")\n",
        ")\n",
        "\n",
        "if CREATE:\n",
        "    if RELEASE:\n",
        "        for video in video_files:\n",
        "            process_video(video)\n",
        "    else:\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            futures = [executor.submit(process_video, video) for video in video_files]\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                try:\n",
        "                    future.result()\n",
        "                except Exception as exc:\n",
        "                    print(f\"An error occurred: {exc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw6y7yDLPvRZ"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZupVk2dN8pX"
      },
      "source": [
        "## Create train.txt for YOLO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "OQf3YXnPawwr"
      },
      "outputs": [],
      "source": [
        "output_file = BASE_DIR / \"train.txt\"  # Name of the output file\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    # Walk through all folders and subfolders\n",
        "    for dirpath, _, filenames in os.walk(TRAIN_IMG_DIR):\n",
        "        for filename in filenames:\n",
        "            if filename.lower().endswith(\".jpg\"):\n",
        "                full_path = os.path.join(dirpath, filename)\n",
        "                f.write(full_path + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_file = BASE_DIR / \"test.txt\"  # Name of the output file\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    # Walk through all folders and subfolders\n",
        "    for dirpath, _, filenames in os.walk(TEST_IMG_DIR):\n",
        "        for filename in filenames:\n",
        "            if filename.lower().endswith(\".jpg\"):\n",
        "                full_path = os.path.join(dirpath, filename)\n",
        "                f.write(full_path + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8yDT_9maBAh"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python \"/content/yolov7/train.py\" \\\n",
        "  --img 640 640 \\\n",
        "  --batch 16 \\\n",
        "  --epochs 100 \\\n",
        "  --cfg \"/yolov7/cfg/training/yolov7.yaml\" \\\n",
        "  --data \"/drive/MyDrive/Colab Notebooks/CV_Project/config.yaml\" \\\n",
        "  --device 0 \\\n",
        "  --weights \"/content/yolov7/yolov7.pt\" \\\n",
        "  --project \"/content/drive/MyDrive/Colab Notebooks/CV_Project\" \\\n",
        "  --name \"run\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SPN4GsCpZC4W",
        "BOgY7NnkQO4S",
        "bB9eG7qbV6jl",
        "hfFX_FAOYTQt",
        "xdYij-suZLWQ",
        "xPNG9tgHZa18",
        "3NNPM9OqZoGw",
        "2GFUTgDpZwlJ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
