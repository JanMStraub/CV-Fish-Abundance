{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPN4GsCpZC4W"
      },
      "source": [
        "# Preliminary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKNTf7L4b2z7"
      },
      "source": [
        "Here we install all imports and other necessary components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1H3gELF5H9o"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccCEcC7sI89U",
        "outputId": "69ccc84a-99bf-439e-fe3d-3cf05d39cd9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio in ./cv_project/lib/python3.12/site-packages (2.35.1)\n",
            "Requirement already satisfied: numpy in ./cv_project/lib/python3.12/site-packages (from imageio) (1.26.4)\n",
            "Requirement already satisfied: pillow>=8.3.2 in ./cv_project/lib/python3.12/site-packages (from imageio) (10.4.0)\n",
            "Requirement already satisfied: numpy in ./cv_project/lib/python3.12/site-packages (1.26.4)\n",
            "Requirement already satisfied: opencv-python in ./cv_project/lib/python3.12/site-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in ./cv_project/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: tqdm in ./cv_project/lib/python3.12/site-packages (4.66.5)\n",
            "Cloning into 'yolov7'...\n",
            "remote: Enumerating objects: 1197, done.\u001b[K\n",
            "remote: Total 1197 (delta 0), reused 0 (delta 0), pack-reused 1197 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1197/1197), 74.23 MiB | 18.11 MiB/s, done.\n",
            "Resolving deltas: 100% (519/519), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install imageio\n",
        "!pip install numpy\n",
        "!pip install opencv-python\n",
        "!pip install tqdm\n",
        "!git clone https://github.com/WongKinYiu/yolov7.git\n",
        "cd yolov7/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE2jk4zOmoPu",
        "outputId": "a9c5728a-4ed8-42a8-f8d6-a60268f21504"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "from IPython.display import display, Image\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from psutil import virtual_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MoiM3ryXiBG",
        "outputId": "b65b2bc9-d6c6-4d71-ebad-17cbdfd6177f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Oct  4 14:56:27 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 4080 ...    Off |   00000000:01:00.0  On |                  N/A |\n",
            "|  0%   36C    P8             21W /  320W |     575MiB /  16376MiB |      1%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A      1309      G   /usr/lib/xorg/Xorg                            186MiB |\n",
            "|    0   N/A  N/A      1782      G   cinnamon                                       50MiB |\n",
            "|    0   N/A  N/A      2623      G   ...erProcess --variations-seed-version        144MiB |\n",
            "|    0   N/A  N/A      3526      G   /usr/lib/firefox/firefox                      158MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Your runtime has 32.7 gigabytes of available RAM\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check what GPU is available\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# Check how much RAM is available\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Mn3ayM5PUR"
      },
      "source": [
        "## Directories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RafB5uuaOSMK"
      },
      "outputs": [],
      "source": [
        "# Configuration Flags\n",
        "CREATE = False # Flag to enable image pipeline\n",
        "SAVE_ORIGINAL = False  # Flag to save original frames\n",
        "RELEASE = False  # Flag to switch between concurrent and sequential processing\n",
        "\n",
        "# Base directory setup\n",
        "BASE_DIR = Path(\"/home/jan/Documents/code/CV-Fish-Abundance\")\n",
        "\n",
        "# Training set directories\n",
        "TRAIN_VIDEO_DIR = BASE_DIR / \"data/fishclef_2015_release/training_set/videos\"\n",
        "TRAIN_GT_DIR = BASE_DIR / \"data/fishclef_2015_release/training_set/gt\"\n",
        "TRAIN_IMG_DIR = BASE_DIR / \"train_img/\"\n",
        "TRAIN_GMM_DIR = BASE_DIR / \"train_gmm/\"\n",
        "TRAIN_OPTICAL_DIR = BASE_DIR / \"train_optical/\"\n",
        "TRAIN_GMM_OPTICAL_DIR = BASE_DIR / \"train_gmm_optical/\"\n",
        "\n",
        "# Test set directories\n",
        "TEST_VIDEO_DIR = BASE_DIR / \"data/fishclef_2015_release/test_set/videos\"\n",
        "TEST_GT_DIR = BASE_DIR / \"data/fishclef_2015_release/test_set/gt\"\n",
        "TEST_IMG_DIR = BASE_DIR / \"test_img/\"\n",
        "TEST_GMM_DIR = BASE_DIR / \"test_gmm/\"\n",
        "TEST_OPTICAL_DIR = BASE_DIR / \"test_optical/\"\n",
        "TEST_GMM_OPTICAL_DIR = BASE_DIR / \"test_gmm_optical/\"\n",
        "\n",
        "# List of species names\n",
        "SPECIES_LIST = [\n",
        "    \"abudefduf vaigiensis\",\n",
        "    \"acanthurus nigrofuscus\",\n",
        "    \"amphiprion clarkii\",\n",
        "    \"chaetodon lununatus\",\n",
        "    \"chaetodon speculum\",\n",
        "    \"chaetodon trifascialis\",\n",
        "    \"chromis chrysura\",\n",
        "    \"dascyllus aruanus\",\n",
        "    \"dascyllus reticulatus\",\n",
        "    \"hemigumnus malapterus\",\n",
        "    \"myripristis kuntee\",\n",
        "    \"neoglyphidodon nigroris\",\n",
        "    \"pempheris vanicolensis\",\n",
        "    \"plectrogly-phidodon dickii\",\n",
        "    \"zebrasoma scopas\",\n",
        "]\n",
        "\n",
        "# Label for unknown species\n",
        "UNKNOWN_LABEL = 15\n",
        "\n",
        "# Frame processing parameters\n",
        "FRAME_RESIZE = (640, 640)\n",
        "\n",
        "# Optical flow parameters\n",
        "FARNEBACK_PARAMS = {\n",
        "    \"pyr_scale\": 0.95,\n",
        "    \"levels\": 10,\n",
        "    \"winsize\": 15,\n",
        "    \"iterations\": 3,\n",
        "    \"poly_n\": 5,\n",
        "    \"poly_sigma\": 1.2,\n",
        "    \"flags\": 0,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMR9UH9jPyNX"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ymY_V2japXC"
      },
      "source": [
        "# Create training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pMM9grDh0b5q"
      },
      "outputs": [],
      "source": [
        "def adjust_gamma(image, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Adjusts the gamma of an image.\n",
        "\n",
        "    Args:\n",
        "        image (np.ndarray): Input image.\n",
        "        gamma (float): Gamma value to adjust (default is 1.0).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Gamma adjusted image.\n",
        "    \"\"\"\n",
        "    invGamma = 1.0 / gamma\n",
        "    table = np.array([(i / 255.0) ** invGamma * 255 for i in range(256)], dtype=\"uint8\")\n",
        "    return cv2.LUT(image, table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_annotation(\n",
        "    name,\n",
        "    annotation_file_path,\n",
        "    bboxes,\n",
        "    image_width,\n",
        "    image_height,\n",
        "    species_key,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates YOLO format annotations for bounding boxes and saves them to files.\n",
        "\n",
        "    Args:\n",
        "        name (str): Name prefix for saved annotation files.\n",
        "        annotation_file_path (Path): Path where annotation files will be saved.\n",
        "        bboxes (list): List of bounding boxes for the frame.\n",
        "        image_width (int): Width of the image.\n",
        "        image_height (int): Height of the image.\n",
        "        species_key (str): Key for accessing species name in bbox dictionary (default is 'fish_species').\n",
        "    \"\"\"\n",
        "    frame_bboxes = {}\n",
        "    \n",
        "    for bbox in bboxes:\n",
        "        frame_id = bbox[\"frame_id\"]\n",
        "        frame_bboxes.setdefault(frame_id, []).append(bbox)\n",
        "\n",
        "    for frame_id, bboxes in frame_bboxes.items():\n",
        "        annotations = []\n",
        "        for fish in bboxes:\n",
        "            fish_species = fish.get(species_key, \"\").lower()\n",
        "            x, y, width, height = (\n",
        "                fish.get(\"x\", 0),\n",
        "                fish.get(\"y\", 0),\n",
        "                fish.get(\"w\", 0),\n",
        "                fish.get(\"h\", 0),\n",
        "            )\n",
        "            x_center = (x + width / 2.0) / image_width\n",
        "            y_center = (y + height / 2.0) / image_height\n",
        "            width /= image_width\n",
        "            height /= image_height\n",
        "            species_index = (\n",
        "                SPECIES_LIST.index(fish_species)\n",
        "                if fish_species in SPECIES_LIST\n",
        "                else UNKNOWN_LABEL\n",
        "            )\n",
        "            annotations.append(\n",
        "                f\"{species_index} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
        "            )\n",
        "\n",
        "        frame_annotation_file = annotation_file_path / f\"{name}_{frame_id:04d}.txt\"\n",
        "        with open(frame_annotation_file, \"w\") as file:\n",
        "            file.write(\"\\n\".join(annotations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_ground_truth(video_path, species_key):\n",
        "    \"\"\"\n",
        "    Extracts ground truth annotations from the corresponding XML file.\n",
        "\n",
        "    Args:\n",
        "        video_path (Path): Path to the video file.\n",
        "        species_key (str): Key for accessing species name in bbox dictionary (default is 'fish_species').\n",
        "\n",
        "    Returns:\n",
        "        list: List of ground truth bounding boxes extracted from XML.\n",
        "    \"\"\"\n",
        "    file_name_without_ext = video_path.stem\n",
        "    gt_xml_path = TEST_GT_DIR / f\"{file_name_without_ext}.xml\"\n",
        "\n",
        "    if not gt_xml_path.exists():\n",
        "        print(f\"Ground truth XML not found: {gt_xml_path}\")\n",
        "        return []\n",
        "\n",
        "    tree = ET.parse(gt_xml_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    ground_truth = []\n",
        "    for frame in root.findall(\"frame\"):\n",
        "        frame_id = int(frame.get(\"id\"))\n",
        "        for obj in frame.findall(\"object\"):\n",
        "            ground_truth.append(\n",
        "                {\n",
        "                    \"frame_id\": frame_id,\n",
        "                    \"fish_species\": obj.get(species_key),\n",
        "                    \"x\": int(obj.get(\"x\")),\n",
        "                    \"y\": int(obj.get(\"y\")),\n",
        "                    \"w\": int(obj.get(\"w\")),\n",
        "                    \"h\": int(obj.get(\"h\")),\n",
        "                }\n",
        "            )\n",
        "    \n",
        "    return ground_truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_gmm(frame, foreground_detector):\n",
        "    \"\"\"\n",
        "    Applies GMM (Gaussian Mixture Model) to detect foreground objects in a frame.\n",
        "\n",
        "    Args:\n",
        "        frame (np.ndarray): Input frame.\n",
        "        foreground_detector (cv2.BackgroundSubtractorMOG2): Foreground detector.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Filtered foreground mask.\n",
        "    \"\"\"\n",
        "    # frame_denoised = cv2.fastNlMeansDenoising(frame, None)\n",
        "    foreground = foreground_detector.apply(frame)\n",
        "    filtered_foreground = cv2.morphologyEx(\n",
        "        foreground, cv2.MORPH_OPEN, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "    )\n",
        "    filtered_foreground = cv2.morphologyEx(\n",
        "        filtered_foreground,\n",
        "        cv2.MORPH_CLOSE,\n",
        "        cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)),\n",
        "    )\n",
        "    # frame_denoised = cv2.fastNlMeansDenoising(filtered_foreground, None)\n",
        "    \n",
        "    # Shadow Removal: Convert shadows to binary foreground\n",
        "    _, filtered_foreground = cv2.threshold(\n",
        "        filtered_foreground, 127, 255, cv2.THRESH_BINARY\n",
        "    )\n",
        "    \n",
        "    return filtered_foreground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_optical_flow(frame, prvs, hsv):\n",
        "    \"\"\"\n",
        "    Computes optical flow using Farneback method and visualizes it in HSV space.\n",
        "\n",
        "    Args:\n",
        "        frame (np.ndarray): Input frame.\n",
        "        prvs (np.ndarray): Previous frame in grayscale.\n",
        "        hsv (np.ndarray): HSV image used for optical flow visualization.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple containing resized BGR image of the flow and next grayscale frame.\n",
        "    \"\"\"\n",
        "    next_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    flow = cv2.calcOpticalFlowFarneback(prvs, next_frame, None, **FARNEBACK_PARAMS)\n",
        "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "    hsv[..., 0] = ang * 180 / np.pi / 2\n",
        "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "    bgr_resized = cv2.resize(bgr, FRAME_RESIZE)\n",
        "    \n",
        "    return bgr_resized, next_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_combination(\n",
        "    frame,\n",
        "    frame_idx,\n",
        "    filtered_foreground,\n",
        "    bgr_resized,\n",
        "    gt_bboxes,\n",
        "    combined_dir,\n",
        "    species_key,\n",
        "    opacity_foreground=0.5,\n",
        "    opacity_optical_flow=0.5,\n",
        "):\n",
        "    \"\"\"\n",
        "    Combines the results of GMM and optical flow with opacity blending, and saves the combined image and annotations.\n",
        "\n",
        "    Args:\n",
        "        frame (np.ndarray): Original frame.\n",
        "        frame_idx (int): Frame index.\n",
        "        filtered_foreground (np.ndarray): Foreground mask obtained from GMM.\n",
        "        bgr_resized (np.ndarray): Optical flow visualization in BGR format.\n",
        "        gt_bboxes (list): List of ground truth bounding boxes.\n",
        "        combined_dir (Path): Directory to save the combined image and annotations.\n",
        "        species_key (str): Key for accessing species name in bbox dictionary (default is 'fish_species').\n",
        "        opacity_foreground (float): Opacity for filtered foreground mask (0 to 1).\n",
        "        opacity_optical_flow (float): Opacity for optical flow visualization (0 to 1).\n",
        "    \"\"\"\n",
        "\n",
        "    combined_frame = np.zeros_like(frame)\n",
        "    grayscale_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    filtered_foreground_normalized = cv2.normalize(\n",
        "        filtered_foreground, None, 0, 255, cv2.NORM_MINMAX\n",
        "    )\n",
        "    blended_foreground = cv2.addWeighted(\n",
        "        grayscale_frame,\n",
        "        1 - opacity_foreground,\n",
        "        filtered_foreground_normalized,\n",
        "        opacity_foreground,\n",
        "        0,\n",
        "    )\n",
        "    blue_channel_optical_flow = bgr_resized[:, :, 0]\n",
        "    blended_optical_flow = cv2.addWeighted(\n",
        "        grayscale_frame,\n",
        "        1 - opacity_optical_flow,\n",
        "        blue_channel_optical_flow,\n",
        "        opacity_optical_flow,\n",
        "        0,\n",
        "    )\n",
        "\n",
        "    combined_frame[:, :, 0] = grayscale_frame  # Grayscale frame\n",
        "    combined_frame[:, :, 1] = blended_foreground  # filtered foreground\n",
        "    combined_frame[:, :, 2] = blended_optical_flow  # Blended optical flow\n",
        "\n",
        "    combined_frame_path = combined_dir / f\"combined_img_{frame_idx:04d}.jpg\"\n",
        "    cv2.imwrite(str(combined_frame_path), combined_frame)\n",
        "\n",
        "    if gt_bboxes:\n",
        "        get_annotation(\n",
        "            \"combined_img\",\n",
        "            combined_dir,\n",
        "            gt_bboxes,\n",
        "            FRAME_RESIZE[0],\n",
        "            FRAME_RESIZE[1],\n",
        "            species_key\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_frame(\n",
        "    frame,\n",
        "    frame1,\n",
        "    frame_idx,\n",
        "    gt_bboxes,\n",
        "    foreground_detector,\n",
        "    prvs,\n",
        "    hsv,\n",
        "    img_dir,\n",
        "    combined_dir,\n",
        "    species_key,\n",
        "):\n",
        "    \"\"\"\n",
        "    Processes a single video frame by applying background subtraction (GMM) and optical flow,\n",
        "    and then combines the results. Optionally saves the original frame, and stores the combined\n",
        "    output along with ground truth annotations.\n",
        "\n",
        "    This function performs the following steps for a given frame:\n",
        "\n",
        "    1. Optionally saves the original frame to a specified directory.\n",
        "    2. Applies Gaussian Mixture Model (GMM) to detect foreground objects in the frame.\n",
        "    3. Computes optical flow between the current and next frame to track movement.\n",
        "    4. Combines the GMM results and optical flow into a final output image.\n",
        "    5. Saves the combined image and associated ground truth annotations to the specified directory.\n",
        "\n",
        "    Args:\n",
        "        frame (numpy.ndarray): The current video frame after resizing and gamma adjustment.\n",
        "        frame1 (numpy.ndarray): The next video frame to compute optical flow.\n",
        "        frame_idx (int): The index of the current frame in the video.\n",
        "        gt_bboxes (list): Ground truth bounding boxes for objects (fish) in the frame.\n",
        "        foreground_detector (cv2.BackgroundSubtractor): Foreground detector based on GMM.\n",
        "        prvs (numpy.ndarray): The previous grayscale frame used for optical flow calculation.\n",
        "        hsv (numpy.ndarray): The HSV image used for visualizing optical flow.\n",
        "        img_dir (Path): Directory to save the original frames.\n",
        "        combined_dir (Path): Directory to save the combined results of GMM and optical flow.\n",
        "        species_key (str): Key for accessing species name in bbox dictionary (default is 'fish_species').\n",
        "\n",
        "    Returns:\n",
        "        next_frame (numpy.ndarray): The grayscale version of the current frame (frame1) for use in the next iteration of optical flow calculation.\n",
        "    \"\"\"\n",
        "    if SAVE_ORIGINAL:\n",
        "        # Save the original frame to the img_dir\n",
        "        img_frame_path = img_dir / f\"img_{frame_idx:04d}.png\"\n",
        "        cv2.imwrite(str(img_frame_path), frame)\n",
        "\n",
        "    # Apply GMM to the frame to detect foreground objects\n",
        "    foreground = apply_gmm(frame, foreground_detector)\n",
        "\n",
        "    # Apply optical flow to the next frame\n",
        "    bgr, next_frame = apply_optical_flow(frame1, prvs, hsv)\n",
        "\n",
        "    # Combine GMM and optical flow results and save the combined image\n",
        "    apply_combination(frame, frame_idx, foreground, bgr, gt_bboxes, combined_dir, species_key)\n",
        "\n",
        "    return next_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_video(video_path):\n",
        "    \"\"\"\n",
        "    Processes a video by applying background subtraction (using Gaussian Mixture Model),\n",
        "    optical flow, and frame adjustments, and saves the processed frames and combined results\n",
        "    along with ground truth annotations.\n",
        "\n",
        "    This function extracts frames from the input video, performs foreground detection using\n",
        "    a Gaussian Mixture Model (GMM), calculates optical flow for movement detection, and\n",
        "    combines these results. The processed frames and combined images are saved in specific\n",
        "    directories. Additionally, it uses ground truth bounding boxes extracted from an\n",
        "    XML file for annotation purposes.\n",
        "\n",
        "    Args:\n",
        "        video_path (Path): Path to the input video file.\n",
        "\n",
        "    Steps:\n",
        "        1. Extract ground truth bounding boxes for the video from the corresponding XML file.\n",
        "        2. Create directories to store processed images and combined results.\n",
        "        3. Open the video and initialize background subtraction (GMM) and optical flow.\n",
        "        4. Process each frame in the video:\n",
        "            - Resize and adjust gamma for frame.\n",
        "            - Apply background subtraction (GMM) for foreground detection.\n",
        "            - Compute optical flow to detect movement.\n",
        "            - Combine the results of GMM and optical flow.\n",
        "            - Save the processed frames and results.\n",
        "        5. Release the video capture object when done.\n",
        "\n",
        "    The function also includes progress tracking using tqdm to visualize the video processing progress.\n",
        "\n",
        "    Parameters:\n",
        "        video_path (Path): Path to the video file being processed.\n",
        "\n",
        "    Returns:\n",
        "        None: The function processes the video, saves results, and does not return anything.\n",
        "    \"\"\"\n",
        "\n",
        "    video_name_short = video_path.stem[-15:]\n",
        "    img_dir = TEST_IMG_DIR / video_name_short\n",
        "    combined_dir = TEST_GMM_OPTICAL_DIR / video_name_short\n",
        "\n",
        "    for directory in [combined_dir]:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "    \n",
        "    if SAVE_ORIGINAL:\n",
        "        for directory in [img_dir]:\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Consider different GT names\n",
        "    species_key = \"\"\n",
        "    if \"train\" in str(combined_dir):\n",
        "        species_key = \"species_name\"\n",
        "    if \"test\" in str(combined_dir):\n",
        "        species_key = \"fish_species\"\n",
        "    \n",
        "    # Extract ground truth bounding boxes from the corresponding XML file\n",
        "    gt_bboxes = extract_ground_truth(video_path, species_key)\n",
        "\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    foreground_detector = cv2.createBackgroundSubtractorMOG2(\n",
        "        history=250, varThreshold=16, detectShadows=True\n",
        "    )\n",
        "\n",
        "    ret, frame1 = cap.read()\n",
        "    if not ret:\n",
        "        print(f\"Failed to read the video file: {video_path}\")\n",
        "        return\n",
        "\n",
        "    prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "    hsv = np.zeros_like(frame1)\n",
        "    hsv[..., 1] = 255\n",
        "    frame_idx = 0\n",
        "\n",
        "    # Process each frame of the video\n",
        "    with tqdm(total=total_frames, desc=f\"Processing {video_name_short}\") as video_pbar:\n",
        "        while ret:\n",
        "            frame = cv2.resize(frame1, FRAME_RESIZE)\n",
        "            frame = adjust_gamma(frame, 1.5)\n",
        "            frame_blurred = cv2.GaussianBlur(frame, (5, 5), 0)\n",
        "\n",
        "            # Process the current frame\n",
        "            next_frame = process_frame(\n",
        "                frame_blurred,\n",
        "                frame1,\n",
        "                frame_idx,\n",
        "                gt_bboxes,\n",
        "                foreground_detector,\n",
        "                prvs,\n",
        "                hsv,\n",
        "                img_dir,\n",
        "                combined_dir,\n",
        "                species_key,\n",
        "            )\n",
        "\n",
        "            video_pbar.update(1)\n",
        "            prvs = next_frame\n",
        "            ret, frame1 = cap.read()\n",
        "            frame_idx += 1\n",
        "\n",
        "    cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Main entry point of the script. Processes either training or test videos.\n",
        "\"\"\"\n",
        "video_files = list(TEST_VIDEO_DIR.glob(\"*.flv\")) + list(\n",
        "    TEST_VIDEO_DIR.glob(\"*.avi\")\n",
        ")\n",
        "\n",
        "if CREATE:\n",
        "    if RELEASE:\n",
        "        for video in video_files:\n",
        "            process_video(video)\n",
        "    else:\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            futures = [executor.submit(process_video, video) for video in video_files]\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                try:\n",
        "                    future.result()\n",
        "                except Exception as exc:\n",
        "                    print(f\"An error occurred: {exc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw6y7yDLPvRZ"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# YOLO Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOLOR ðŸš€ v0.1-128-ga207844 torch 2.4.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4080 SUPER, 15951.875MB)\n",
            "\n",
            "Namespace(weights='/home/jan/Documents/code/CV-Fish-Abundance/yolo_files', cfg='./cfg/training/yolov7.yaml', data='/home/jan/Documents/code/CV-Fish-Abundance/yolo_files/config.yaml', hyp='data/hyp.scratch.p5.yaml', epochs=300, batch_size=64, img_size=[240, 240], rect=False, resume=False, nosave=False, notest=False, noautoanchor=False, evolve=False, bucket='', cache_images=False, image_weights=False, device='0', multi_scale=False, single_cls=False, adam=False, sync_bn=False, local_rank=-1, workers=8, project='runs/train', entity=None, name='exp', exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias='latest', freeze=[0], v5_metric=False, world_size=1, global_rank=-1, save_dir='runs/train/exp', total_batch_size=64)\n",
            "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n",
            "\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n",
            "Overriding model.yaml nc=80 with nc=16\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            "  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            "  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 12                -1  1         0  models.common.MP                        []                            \n",
            " 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n",
            " 25                -1  1         0  models.common.MP                        []                            \n",
            " 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
            " 38                -1  1         0  models.common.MP                        []                            \n",
            " 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
            " 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n",
            " 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
            " 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
            " 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
            " 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
            " 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n",
            " 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
            " 76                -1  1         0  models.common.MP                        []                            \n",
            " 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
            " 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
            " 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n",
            " 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
            " 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 89                -1  1         0  models.common.MP                        []                            \n",
            " 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n",
            " 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n",
            " 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            "100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            "101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n",
            "102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n",
            "103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n",
            "104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n",
            "105   [102, 103, 104]  1    115066  models.yolo.IDetect                     [16, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n",
            "/home/jan/Documents/code/CV-Fish-Abundance/cv_project/lib/python3.12/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 415 layers, 37277466 parameters, 37277466 gradients, 105.4 GFLOPS\n",
            "\n",
            "Scaled weight_decay = 0.0005\n",
            "Optimizer groups: 95 .bias, 95 conv.weight, 98 other\n",
            "WARNING: --img-size 240 must be multiple of max stride 32, updating to 256\n",
            "WARNING: --img-size 240 must be multiple of max stride 32, updating to 256\n",
            "/home/jan/Documents/code/CV-Fish-Abundance/yolov7/utils/datasets.py:392: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  cache, exists = torch.load(cache_path), True  # load\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/home/jan/Documents/code/CV-Fish-Abundance/train_combined/#2010\u001b[0m\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/jan/Documents/code/CV-Fish-Abundance/val_combined/#20110306\u001b[0m\n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 5.58, Best Possible Recall (BPR) = 1.0000\n",
            "/home/jan/Documents/code/CV-Fish-Abundance/yolov7/train.py:299: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = amp.GradScaler(enabled=cuda)\n",
            "Image sizes 256 train, 256 test\n",
            "Using 8 dataloader workers\n",
            "Logging results to runs/train/exp\n",
            "Starting training for 300 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "  0%|                                                   | 0/102 [00:00<?, ?it/s]/home/jan/Documents/code/CV-Fish-Abundance/yolov7/train.py:360: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(enabled=cuda):\n",
            "     0/299        2G   0.07696  0.003279   0.04193    0.1222        65       256\n",
            "               Class      Images      Labels           P           R      mAP@.5\n",
            "                 all       18827        8647    1.88e-06     0.00029    4.97e-08    4.97e-09\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     1/299      7.8G   0.07548    0.0027   0.03832    0.1165        60       256\n",
            "               Class      Images      Labels           P           R      mAP@.5\n",
            "                 all       18827        8647    1.01e-05     0.00145    2.58e-07    6.95e-08\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "     2/299     9.84G    0.0744  0.002507   0.03642    0.1133        46       256\n",
            "               Class      Images      Labels           P           R      mAP@.5^C\n",
            "               Class      Images      Labels           P           R      mAP@.5\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/jan/Documents/code/CV-Fish-Abundance/yolov7/train.py\", line 616, in <module>\n",
            "    train(hyp, opt, device, tb_writer)\n",
            "  File \"/home/jan/Documents/code/CV-Fish-Abundance/yolov7/train.py\", line 415, in train\n",
            "    results, maps, times = test.test(data_dict,\n",
            "                           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/jan/Documents/code/CV-Fish-Abundance/yolov7/test.py\", line 115, in test\n",
            "    t0 += time_synchronized() - t\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/jan/Documents/code/CV-Fish-Abundance/yolov7/utils/torch_utils.py\", line 92, in time_synchronized\n",
            "    torch.cuda.synchronize()\n",
            "  File \"/home/jan/Documents/code/CV-Fish-Abundance/cv_project/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 892, in synchronize\n",
            "    return torch._C._cuda_synchronize()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python train.py --workers 8 --device 0 --batch-size 16 --data \"/home/jan/Documents/code/CV-Fish-Abundance/yolo_files/config.yaml\" --img 640 640 --cfg \"/home/jan/Documents/code/CV-Fish-Abundance/yolo_files/yolov7_custom_fishdetection.yaml\" --weights \"/home/jan/Documents/code/CV-Fish-Abundance/yolo_files/best.pt\" --hyp \"/home/jan/Documents/code/CV-Fish-Abundance/yolo_files/hyp.scratch.custom_fishdetection.yaml\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SPN4GsCpZC4W",
        "BOgY7NnkQO4S",
        "bB9eG7qbV6jl",
        "hfFX_FAOYTQt",
        "xdYij-suZLWQ",
        "xPNG9tgHZa18",
        "3NNPM9OqZoGw",
        "2GFUTgDpZwlJ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cv_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
