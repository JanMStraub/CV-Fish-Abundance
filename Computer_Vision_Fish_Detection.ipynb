{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPN4GsCpZC4W"
      },
      "source": [
        "# Preliminary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKNTf7L4b2z7"
      },
      "source": [
        "Here we install all imports and other necessary components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1H3gELF5H9o"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccCEcC7sI89U",
        "outputId": "69ccc84a-99bf-439e-fe3d-3cf05d39cd9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.2.80 ðŸš€ Python-3.10.1 torch-2.4.0 CPU (Apple M1)\n",
            "Setup complete âœ… (8 CPUs, 16.0 GB RAM, 284.5/460.4 GB disk)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "!pip install imageio\n",
        "!pip install numpy\n",
        "!pip install opencv-python\n",
        "!pip install tqdm\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE2jk4zOmoPu",
        "outputId": "a9c5728a-4ed8-42a8-f8d6-a60268f21504"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "display.clear_output()\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MoiM3ryXiBG",
        "outputId": "b65b2bc9-d6c6-4d71-ebad-17cbdfd6177f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: nvidia-smi\n"
          ]
        }
      ],
      "source": [
        "# Check what GPU is available\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2Mn3ayM5PUR"
      },
      "source": [
        "## Directories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RafB5uuaOSMK"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\"/Users/jan/Documents/code/cv/project\")\n",
        "VIDEO_DIR = BASE_DIR / \"data/fishclef_2015_release/training_set/videos\"\n",
        "GT_DIR = BASE_DIR / \"data/fishclef_2015_release/training_set/gt\"\n",
        "IMG_DIR = BASE_DIR / \"train_img/\"\n",
        "GMM_DIR = BASE_DIR / \"train_gmm/\"\n",
        "OPTICAL_DIR = BASE_DIR / \"train_optical/\"\n",
        "GMM_OPTICAL_DIR = BASE_DIR / \"train_gmm_optical/\"\n",
        "\n",
        "SPECIES_LIST = [\n",
        "    \"abudefduf vaigiensis\",\n",
        "    \"acanthurus nigrofuscus\",\n",
        "    \"amphiprion clarkii\",\n",
        "    \"chaetodon lununatus\",\n",
        "    \"chaetodon speculum\",\n",
        "    \"chaetodon trifascialis\",\n",
        "    \"chromis chrysura\",\n",
        "    \"dascyllus aruanus\",\n",
        "    \"dascyllus reticulatus\",\n",
        "    \"hemigumnus malapterus\",\n",
        "    \"myripristis kuntee\",\n",
        "    \"neoglyphidodon nigroris\",\n",
        "    \"pempheris vanicolensis\",\n",
        "    \"plectrogly-phidodon dickii\",\n",
        "    \"zebrasoma scopas\",\n",
        "]\n",
        "\n",
        "UNKNOWN_LABEL = 15\n",
        "\n",
        "FOREGROUND_DETECTOR_PARAMS = {\n",
        "    \"history\": 250,\n",
        "    \"varThreshold\": 16,\n",
        "    \"detectShadows\": True,\n",
        "}\n",
        "BLOB_ANALYSIS_PARAMS = {\"min_area\": 200}\n",
        "STRUCTURING_ELEMENT_OPEN = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "STRUCTURING_ELEMENT_CLOSE = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "FRAME_RESIZE = (640, 640)\n",
        "FRAME_ADJUST_GAMMA = 1.5\n",
        "FARNEBACK_PARAMS = {\n",
        "    \"pyr_scale\": 0.5,\n",
        "    \"levels\": 3,\n",
        "    \"winsize\": 15,\n",
        "    \"iterations\": 3,\n",
        "    \"poly_n\": 5,\n",
        "    \"poly_sigma\": 1.2,\n",
        "    \"flags\": 0,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMR9UH9jPyNX"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ymY_V2japXC"
      },
      "source": [
        "# Create training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "1aWQB5tt0Req"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(IMG_DIR) == False:\n",
        "    os.mkdir(IMG_DIR)\n",
        "\n",
        "if os.path.exists(GMM_DIR) == False:\n",
        "    os.mkdir(GMM_DIR)\n",
        "\n",
        "if os.path.exists(OPTICAL_DIR) == False:\n",
        "    os.mkdir(OPTICAL_DIR)\n",
        "\n",
        "if os.path.exists(GMM_OPTICAL_DIR) == False:\n",
        "    os.mkdir(GMM_OPTICAL_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pMM9grDh0b5q"
      },
      "outputs": [],
      "source": [
        "def adjust_gamma(image, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Adjust the gamma of an image.\n",
        "\n",
        "    Parameters:\n",
        "    - image (numpy.ndarray): The input image on which gamma correction is to be applied.\n",
        "    - gamma (float): The gamma value for correction. Default is 1.0. Values less than 1.0 will make the image darker,\n",
        "                     while values greater than 1.0 will make the image lighter.\n",
        "\n",
        "    Returns:\n",
        "    - numpy.ndarray: The gamma-corrected image.\n",
        "    \"\"\"\n",
        "    # Calculate the inverse of the gamma value\n",
        "    invGamma = 1.0 / gamma\n",
        "\n",
        "    # Build a lookup table mapping pixel values [0, 255] to their adjusted gamma values\n",
        "    table = np.array([(i / 255.0) ** invGamma * 255 for i in range(256)], dtype=\"uint8\")\n",
        "\n",
        "    # Apply the gamma correction using the lookup table\n",
        "    return cv2.LUT(image, table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_gmm_annotation(annotation_filename, bboxes, image_width, image_height):\n",
        "    \"\"\"\n",
        "    Save annotations in YOLO format for Gaussian Mixture Model (GMM) detected bounding boxes.\n",
        "\n",
        "    Parameters:\n",
        "    - annotation_filename (str): The file to save annotations.\n",
        "    - bboxes (list of tuples): List of bounding boxes, where each bounding box is represented as a tuple (x, y, width, height).\n",
        "    - image_width (int): Width of the image.\n",
        "    - image_height (int): Height of the image.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Open the annotation file in write mode\n",
        "    with open(annotation_filename, \"w\") as file:\n",
        "        # Iterate over each bounding box\n",
        "        for x, y, width, height in bboxes:\n",
        "            # Normalize the coordinates\n",
        "            x_center = (x + width / 2.0) / image_width\n",
        "            y_center = (y + height / 2.0) / image_height\n",
        "            width /= image_width\n",
        "            height /= image_height\n",
        "\n",
        "            # Write the normalized coordinates to the file in YOLO format\n",
        "            file.write(f\"0 {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_annotation(name, annotation_file_path, bboxes, image_width, image_height):\n",
        "    \"\"\"\n",
        "    Save annotations in YOLO format for each frame.\n",
        "\n",
        "    Parameters:\n",
        "    - name (str): Base name for the annotation files.\n",
        "    - annotation_file_path (Path): Path object representing the directory to save annotation files.\n",
        "    - bboxes (list of dict): List of bounding boxes, where each bounding box is represented as a dictionary with keys 'frame_id', 'fish_species', 'x', 'y', 'w', 'h'.\n",
        "    - image_width (int): Width of the image.\n",
        "    - image_height (int): Height of the image.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Group bounding boxes by frame_id\n",
        "    frame_bboxes = {}\n",
        "    for bbox in bboxes:\n",
        "        frame_id = bbox[\"frame_id\"]\n",
        "        frame_bboxes.setdefault(frame_id, []).append(bbox)\n",
        "\n",
        "    # Iterate over each frame and save annotations\n",
        "    for frame_id, bboxes in frame_bboxes.items():\n",
        "        # Create a unique file name for each frame\n",
        "        frame_annotation_file = annotation_file_path / f\"{name}_{frame_id:04d}.txt\"\n",
        "\n",
        "        # Open the annotation file in write mode\n",
        "        with open(frame_annotation_file, \"w\") as file:\n",
        "            # Iterate over each bounding box in the frame\n",
        "            for fish in bboxes:\n",
        "                fish_species = fish.get(\"fish_species\", \"\").lower()\n",
        "                x, y, width, height = (\n",
        "                    fish.get(\"x\", 0),\n",
        "                    fish.get(\"y\", 0),\n",
        "                    fish.get(\"w\", 0),\n",
        "                    fish.get(\"h\", 0),\n",
        "                )\n",
        "\n",
        "                # Normalize the coordinates\n",
        "                x_center = (x + width / 2.0) / image_width\n",
        "                y_center = (y + height / 2.0) / image_height\n",
        "                width /= image_width\n",
        "                height /= image_height\n",
        "\n",
        "                # Determine the species index\n",
        "                species_index = (\n",
        "                    SPECIES_LIST.index(fish_species)\n",
        "                    if fish_species in SPECIES_LIST\n",
        "                    else UNKNOWN_LABEL\n",
        "                )\n",
        "\n",
        "                # Write the normalized coordinates to the file in YOLO format\n",
        "                file.write(\n",
        "                    f\"{species_index} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\"\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_ground_truth(video_path):\n",
        "    \"\"\"\n",
        "    Extract ground truth annotations from an XML file corresponding to a video.\n",
        "\n",
        "    Parameters:\n",
        "    - video_path (Path): Path object representing the path to the video file.\n",
        "\n",
        "    Returns:\n",
        "    - list of dict: A list of dictionaries, where each dictionary contains the ground truth annotations for a frame.\n",
        "      Each dictionary has the following keys:\n",
        "        - frame_id (int): The ID of the frame.\n",
        "        - fish_species (str): The species of the fish.\n",
        "        - x (int): The x-coordinate of the bounding box.\n",
        "        - y (int): The y-coordinate of the bounding box.\n",
        "        - w (int): The width of the bounding box.\n",
        "        - h (int): The height of the bounding box.\n",
        "    \"\"\"\n",
        "    # Extract the file name without extension from the video path\n",
        "    file_name_without_ext = video_path.stem\n",
        "\n",
        "    # Construct the path to the ground truth XML file\n",
        "    gt_xml_path = GT_DIR / f\"{file_name_without_ext}.xml\"\n",
        "\n",
        "    # Check if the ground truth XML file exists\n",
        "    if not gt_xml_path.exists():\n",
        "        print(f\"Ground truth XML not found: {gt_xml_path}\")\n",
        "        return []\n",
        "\n",
        "    # Parse the XML file\n",
        "    tree = ET.parse(gt_xml_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Initialize an empty list to store ground truth annotations\n",
        "    ground_truth = []\n",
        "\n",
        "    # Iterate over each frame element in the XML\n",
        "    for frame in root.findall(\"frame\"):\n",
        "        frame_id = int(frame.get(\"id\"))\n",
        "\n",
        "        # Iterate over each object element within the frame\n",
        "        for obj in frame.findall(\"object\"):\n",
        "            # Append the ground truth annotation to the list\n",
        "            ground_truth.append(\n",
        "                {\n",
        "                    \"frame_id\": frame_id,\n",
        "                    \"fish_species\": obj.get(\"fish_species\"),\n",
        "                    \"x\": int(obj.get(\"x\")),\n",
        "                    \"y\": int(obj.get(\"y\")),\n",
        "                    \"w\": int(obj.get(\"w\")),\n",
        "                    \"h\": int(obj.get(\"h\")),\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return ground_truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_gmm(frame, frame_idx, gmm_dir, foreground_detector):\n",
        "    \"\"\"\n",
        "    Apply Gaussian Mixture Model (GMM) to a video frame to detect foreground objects and save the results.\n",
        "\n",
        "    Parameters:\n",
        "    - frame (numpy.ndarray): The input video frame.\n",
        "    - frame_idx (int): The index of the current frame.\n",
        "    - gmm_dir (Path): Path object representing the directory to save GMM results.\n",
        "    - foreground_detector (cv2.BackgroundSubtractor): The foreground detector object.\n",
        "\n",
        "    Returns:\n",
        "    - numpy.ndarray: The processed foreground mask.\n",
        "    \"\"\"\n",
        "    # Apply the foreground detector to the frame\n",
        "    foreground = foreground_detector.apply(frame)\n",
        "\n",
        "    # Apply morphological opening to remove noise\n",
        "    filtered_foreground = cv2.morphologyEx(\n",
        "        foreground, cv2.MORPH_OPEN, STRUCTURING_ELEMENT_OPEN\n",
        "    )\n",
        "\n",
        "    # Apply morphological closing to fill gaps\n",
        "    filtered_foreground = cv2.morphologyEx(\n",
        "        filtered_foreground, cv2.MORPH_CLOSE, STRUCTURING_ELEMENT_CLOSE\n",
        "    )\n",
        "\n",
        "    # Find contours in the filtered foreground mask\n",
        "    contours, _ = cv2.findContours(\n",
        "        filtered_foreground, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "    )\n",
        "\n",
        "    # Filter contours based on minimum area and compute bounding boxes\n",
        "    bboxes = [\n",
        "        cv2.boundingRect(c)\n",
        "        for c in contours\n",
        "        if cv2.contourArea(c) >= BLOB_ANALYSIS_PARAMS[\"min_area\"]\n",
        "    ]\n",
        "\n",
        "    # Save the filtered foreground mask as an image\n",
        "    gmm_frame_path = gmm_dir / f\"gmm_img_{frame_idx:04d}.png\"\n",
        "    cv2.imwrite(str(gmm_frame_path), filtered_foreground)\n",
        "\n",
        "    # Save the bounding boxes as annotations\n",
        "    gmm_annotation_path = gmm_dir / f\"gmm_img_{frame_idx:04d}.txt\"\n",
        "    if bboxes:\n",
        "        save_gmm_annotation(\n",
        "            gmm_annotation_path, bboxes, FRAME_RESIZE[0], FRAME_RESIZE[1]\n",
        "        )\n",
        "    else:\n",
        "        # Create an empty annotation file if no bounding boxes are found\n",
        "        gmm_annotation_path.touch()\n",
        "\n",
        "    return filtered_foreground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_optical_flow(frame, frame_idx, prvs, hsv, flow_dir):\n",
        "    \"\"\"\n",
        "    Apply Farneback optical flow to a video frame and save the results.\n",
        "\n",
        "    Parameters:\n",
        "    - frame (numpy.ndarray): The current video frame.\n",
        "    - frame_idx (int): The index of the current frame.\n",
        "    - prvs (numpy.ndarray): The previous grayscale frame.\n",
        "    - hsv (numpy.ndarray): The HSV image used for visualizing the optical flow.\n",
        "    - flow_dir (Path): Path object representing the directory to save optical flow results.\n",
        "\n",
        "    Returns:\n",
        "    - tuple: A tuple containing:\n",
        "        - bgr_resized (numpy.ndarray): The resized BGR image representing the optical flow.\n",
        "        - next_frame (numpy.ndarray): The next grayscale frame.\n",
        "    \"\"\"\n",
        "    # Convert the current frame to grayscale\n",
        "    next_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculate the optical flow using Farneback method\n",
        "    flow = cv2.calcOpticalFlowFarneback(prvs, next_frame, None, **FARNEBACK_PARAMS)\n",
        "\n",
        "    # Compute the magnitude and angle of the flow\n",
        "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "    # Set the hue of the HSV image based on the angle of the flow\n",
        "    hsv[..., 0] = ang * 180 / np.pi / 2\n",
        "\n",
        "    # Set the value of the HSV image based on the normalized magnitude of the flow\n",
        "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "    # Convert the HSV image to BGR for visualization\n",
        "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    # Resize the BGR image to match the desired frame size (640x640)\n",
        "    bgr_resized = cv2.resize(bgr, FRAME_RESIZE)\n",
        "\n",
        "    # Construct the path to save the optical flow frame\n",
        "    flow_frame_path = flow_dir / f\"flow_img_{frame_idx:04d}.png\"\n",
        "\n",
        "    # Save the resized BGR image to the specified directory\n",
        "    cv2.imwrite(str(flow_frame_path), bgr_resized)\n",
        "\n",
        "    return bgr_resized, next_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_combination(\n",
        "    frame, frame_idx, filtered_foreground, bgr_resized, gt_bboxes, combined_dir\n",
        "):\n",
        "    \"\"\"\n",
        "    Combine GMM and Optical Flow images, save the combined image, and save ground truth annotations.\n",
        "\n",
        "    Parameters:\n",
        "    - frame (numpy.ndarray): The original video frame.\n",
        "    - frame_idx (int): The index of the current frame.\n",
        "    - filtered_foreground (numpy.ndarray): The foreground mask obtained from GMM.\n",
        "    - bgr_resized (numpy.ndarray): The resized BGR image obtained from optical flow.\n",
        "    - gt_bboxes (list of dict): List of ground truth bounding boxes for the frame.\n",
        "    - combined_dir (Path): Path object representing the directory to save combined results.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Initialize a blank image with the same shape as the original frame\n",
        "    combined_frame = np.zeros_like(frame)\n",
        "\n",
        "    # Combine the filtered foreground mask into the green channel\n",
        "    combined_frame[:, :, 1] = filtered_foreground\n",
        "\n",
        "    # Combine the blue channel of the resized BGR image into the red channel\n",
        "    combined_frame[:, :, 2] = bgr_resized[:, :, 0]  # Use resized bgr\n",
        "\n",
        "    # Construct the path to save the combined image\n",
        "    combined_frame_path = combined_dir / f\"combined_img_{frame_idx:04d}.png\"\n",
        "\n",
        "    # Save the combined image to the specified directory\n",
        "    cv2.imwrite(str(combined_frame_path), combined_frame)\n",
        "\n",
        "    # Construct the path to save the ground truth annotations\n",
        "    combined_annotation_path = combined_dir / f\"combined_img_{frame_idx:04d}.txt\"\n",
        "    name = \"combined_img\"\n",
        "\n",
        "    # Save the ground truth annotations if they exist\n",
        "    if gt_bboxes:\n",
        "        save_annotation(\n",
        "            name,\n",
        "            combined_dir,\n",
        "            gt_bboxes,\n",
        "            FRAME_RESIZE[0],\n",
        "            FRAME_RESIZE[1],\n",
        "        )\n",
        "    else:\n",
        "        # Create an empty annotation file if no ground truth bounding boxes are found\n",
        "        combined_annotation_path.touch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_frame(\n",
        "    frame,\n",
        "    frame1,\n",
        "    frame_idx,\n",
        "    gt_bboxes,\n",
        "    foreground_detector,\n",
        "    prvs,\n",
        "    hsv,\n",
        "    img_dir,\n",
        "    gmm_dir,\n",
        "    flow_dir,\n",
        "    combined_dir,\n",
        "):\n",
        "    \"\"\"\n",
        "    Process a video frame by applying GMM and Optical Flow, and save the results.\n",
        "\n",
        "    Parameters:\n",
        "    - frame (numpy.ndarray): The current video frame.\n",
        "    - frame1 (numpy.ndarray): The next video frame for optical flow calculation.\n",
        "    - frame_idx (int): The index of the current frame.\n",
        "    - gt_bboxes (list of dict): List of ground truth bounding boxes for the frame.\n",
        "    - foreground_detector (cv2.BackgroundSubtractor): The foreground detector object.\n",
        "    - prvs (numpy.ndarray): The previous grayscale frame for optical flow calculation.\n",
        "    - hsv (numpy.ndarray): The HSV image used for visualizing the optical flow.\n",
        "    - img_dir (Path): Path object representing the directory to save original frames.\n",
        "    - gmm_dir (Path): Path object representing the directory to save GMM results.\n",
        "    - flow_dir (Path): Path object representing the directory to save optical flow results.\n",
        "    - combined_dir (Path): Path object representing the directory to save combined results.\n",
        "\n",
        "    Returns:\n",
        "    - numpy.ndarray: The next grayscale frame for optical flow calculation.\n",
        "    \"\"\"\n",
        "    # Save the original frame to the img_dir\n",
        "    img_frame_path = img_dir / f\"img_{frame_idx:04d}.png\"\n",
        "    cv2.imwrite(str(img_frame_path), frame)\n",
        "\n",
        "    # Save annotations for the original frame (train_img)\n",
        "    img_annotation_path = img_dir / f\"img_{frame_idx:04d}.txt\"\n",
        "    name = \"img\"\n",
        "    if gt_bboxes:\n",
        "        # Save ground truth annotations if they exist\n",
        "        save_annotation(\n",
        "            name,\n",
        "            img_dir,\n",
        "            gt_bboxes,\n",
        "            FRAME_RESIZE[0],\n",
        "            FRAME_RESIZE[1],\n",
        "        )\n",
        "    else:\n",
        "        # Create an empty annotation file if no ground truth bounding boxes are found\n",
        "        img_annotation_path.touch()\n",
        "\n",
        "    # Apply GMM to the frame to detect foreground objects\n",
        "    foreground = apply_gmm(frame, frame_idx, gmm_dir, foreground_detector)\n",
        "\n",
        "    # Apply optical flow to the next frame\n",
        "    bgr, next_frame = apply_optical_flow(frame1, frame_idx, prvs, hsv, flow_dir)\n",
        "\n",
        "    # Combine GMM and optical flow results and save the combined image\n",
        "    apply_combination(frame, frame_idx, foreground, bgr, gt_bboxes, combined_dir)\n",
        "\n",
        "    return next_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_video(video_path):\n",
        "    \"\"\"\n",
        "    Process a video to extract ground truth, apply GMM and optical flow, and save the results.\n",
        "\n",
        "    Parameters:\n",
        "    - video_path (Path): Path object representing the path to the video file.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Extract the last 15 characters of the video file name (without extension) to use as a directory name\n",
        "    video_name_short = video_path.stem[-15:]\n",
        "\n",
        "    # Define directories for saving images, GMM results, optical flow results, and combined results\n",
        "    img_dir = IMG_DIR / video_name_short\n",
        "    gmm_dir = GMM_DIR / video_name_short\n",
        "    flow_dir = OPTICAL_DIR / video_name_short\n",
        "    combined_dir = GMM_OPTICAL_DIR / video_name_short\n",
        "\n",
        "    # Create the directories if they do not exist\n",
        "    for directory in [img_dir, gmm_dir, flow_dir, combined_dir]:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Extract ground truth bounding boxes from the corresponding XML file\n",
        "    gt_bboxes = extract_ground_truth(video_path)\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "\n",
        "    # Get the total number of frames in the video\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Create a foreground detector using MOG2\n",
        "    foreground_detector = cv2.createBackgroundSubtractorMOG2(\n",
        "        **FOREGROUND_DETECTOR_PARAMS\n",
        "    )\n",
        "\n",
        "    # Read the first frame of the video\n",
        "    ret, frame1 = cap.read()\n",
        "\n",
        "    # Check if the video file was read successfully\n",
        "    if not ret:\n",
        "        print(f\"Failed to read the video file: {video_path}\")\n",
        "        return\n",
        "\n",
        "    # Convert the first frame to grayscale for optical flow calculation\n",
        "    prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Initialize an HSV image for visualizing optical flow\n",
        "    hsv = np.zeros_like(frame1)\n",
        "    hsv[..., 1] = 255\n",
        "\n",
        "    # Initialize the frame index\n",
        "    frame_idx = 0\n",
        "\n",
        "    # Process each frame of the video\n",
        "    with tqdm(total=total_frames, desc=f\"Processing {video_name_short}\") as video_pbar:\n",
        "        while ret:\n",
        "            # Resize the frame and adjust its gamma\n",
        "            frame = cv2.resize(frame1, FRAME_RESIZE)\n",
        "            frame = adjust_gamma(frame, FRAME_ADJUST_GAMMA)\n",
        "\n",
        "            # Process the current frame\n",
        "            next_frame = process_frame(\n",
        "                frame,\n",
        "                frame1,\n",
        "                frame_idx,\n",
        "                gt_bboxes,\n",
        "                foreground_detector,\n",
        "                prvs,\n",
        "                hsv,\n",
        "                img_dir,\n",
        "                gmm_dir,\n",
        "                flow_dir,\n",
        "                combined_dir,\n",
        "            )\n",
        "\n",
        "            # Update the progress bar\n",
        "            video_pbar.update(1)\n",
        "\n",
        "            # Update the previous frame for optical flow calculation\n",
        "            prvs = next_frame\n",
        "\n",
        "            # Read the next frame of the video\n",
        "            ret, frame1 = cap.read()\n",
        "\n",
        "            # Increment the frame index\n",
        "            frame_idx += 1\n",
        "\n",
        "    # Release the video capture object\n",
        "    cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Main function to process all video files in the specified directory.\n",
        "\n",
        "This function searches for video files with .flv and .avi extensions in the VIDEO_DIR,\n",
        "and processes each video file using the process_video function.\n",
        "\"\"\"\n",
        "# Get a list of all .flv and .avi video files in the VIDEO_DIR\n",
        "video_files = list(VIDEO_DIR.glob(\"*.flv\")) + list(VIDEO_DIR.glob(\"*.avi\"))\n",
        "\n",
        "# Iterate over each video file and process it\n",
        "for video in video_files:\n",
        "    process_video(video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw6y7yDLPvRZ"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZupVk2dN8pX"
      },
      "source": [
        "## Create train.txt for YOLO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQf3YXnPawwr"
      },
      "outputs": [],
      "source": [
        "output_file = \"train.txt\"  # Name of the output file\n",
        "\n",
        "with open(output_file, \"w\") as out_file:\n",
        "    # Iterate over all files in the directory\n",
        "    for root, dirs, files in os.walk(IMG_DIR):\n",
        "        for file_name in files:\n",
        "            # Check if the file has a .jpg extension\n",
        "            if file_name.lower().endswith(\".jpg\"):\n",
        "                file_path = os.path.join(root, file_name)\n",
        "                out_file.write(f\"{IMG_DIR}{file_name}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8yDT_9maBAh"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdYij-suZLWQ"
      },
      "source": [
        "# Classification using YOLO (TODO)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPNG9tgHZa18"
      },
      "source": [
        "## Custom structures for YOLO detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "_JD4T8gtZcH2"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Structure' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBOX\u001b[39;00m(\u001b[43mStructure\u001b[49m):\n\u001b[1;32m      2\u001b[0m     _fields_ \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, c_float), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, c_float), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, c_float), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m, c_float)]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDETECTION\u001b[39;00m(Structure):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Structure' is not defined"
          ]
        }
      ],
      "source": [
        "class BOX(Structure):\n",
        "    _fields_ = [(\"x\", c_float), (\"y\", c_float), (\"w\", c_float), (\"h\", c_float)]\n",
        "\n",
        "\n",
        "class DETECTION(Structure):\n",
        "    _fields_ = [\n",
        "        (\"bbox\", BOX),\n",
        "        (\"classes\", c_int),\n",
        "        (\"prob\", POINTER(c_float)),\n",
        "        (\"mask\", POINTER(c_float)),\n",
        "        (\"objectness\", c_float),\n",
        "        (\"sort_class\", c_int),\n",
        "    ]\n",
        "\n",
        "\n",
        "class IMAGE(Structure):\n",
        "    _fields_ = [(\"w\", c_int), (\"h\", c_int), (\"c\", c_int), (\"data\", POINTER(c_float))]\n",
        "\n",
        "\n",
        "class METADATA(Structure):\n",
        "    _fields_ = [(\"classes\", c_int), (\"names\", POINTER(c_char_p))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NNPM9OqZoGw"
      },
      "source": [
        "## Load and set up darknet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "iFBmwBl2Zlq5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'CDLL' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load darknet library\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m lib \u001b[38;5;241m=\u001b[39m \u001b[43mCDLL\u001b[49m(Path(\u001b[38;5;18m__file__\u001b[39m)\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../darknet/libdarknet.so\u001b[39m\u001b[38;5;124m\"\u001b[39m, RTLD_GLOBAL)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Set up function arguments and return types\u001b[39;00m\n\u001b[1;32m      5\u001b[0m lib\u001b[38;5;241m.\u001b[39mnetwork_width\u001b[38;5;241m.\u001b[39margtypes \u001b[38;5;241m=\u001b[39m [c_void_p]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CDLL' is not defined"
          ]
        }
      ],
      "source": [
        "# Load darknet library\n",
        "lib = CDLL(Path(__file__).parent / \"../darknet/libdarknet.so\", RTLD_GLOBAL)\n",
        "\n",
        "# Set up function arguments and return types\n",
        "lib.network_width.argtypes = [c_void_p]\n",
        "lib.network_width.restype = c_int\n",
        "lib.network_height.argtypes = [c_void_p]\n",
        "lib.network_height.restype = c_int\n",
        "lib.get_metadata.argtypes = [c_char_p]\n",
        "lib.get_metadata.restype = METADATA\n",
        "lib.load_network.argtypes = [c_char_p, c_char_p, c_int]\n",
        "lib.load_network.restype = c_void_p\n",
        "lib.load_image_color.argtypes = [c_char_p, c_int, c_int]\n",
        "lib.load_image_color.restype = IMAGE\n",
        "lib.network_predict_image.argtypes = [c_void_p, IMAGE]\n",
        "lib.network_predict_image.restype = POINTER(c_float)\n",
        "lib.get_network_boxes.argtypes = [\n",
        "    c_void_p,\n",
        "    c_int,\n",
        "    c_int,\n",
        "    c_float,\n",
        "    c_float,\n",
        "    POINTER(c_int),\n",
        "    c_int,\n",
        "    POINTER(c_int),\n",
        "]\n",
        "lib.get_network_boxes.restype = POINTER(DETECTION)\n",
        "lib.do_nms_obj.argtypes = [POINTER(DETECTION), c_int, c_int, c_float]\n",
        "lib.free_image.argtypes = [IMAGE]\n",
        "lib.free_detections.argtypes = [POINTER(DETECTION), c_int]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GFUTgDpZwlJ"
      },
      "source": [
        "## Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxW_AGC4ZsZI"
      },
      "outputs": [],
      "source": [
        "def detect(net, meta, image, thresh=0.25, hier_thresh=0.5, nms=0.45):\n",
        "    \"\"\"\n",
        "    Performs object detection on a given image using the YOLO model.\n",
        "\n",
        "    Parameters:\n",
        "    - net: YOLO network object.\n",
        "    - meta: Metadata object containing class information.\n",
        "    - image: Path to the image file.\n",
        "    - thresh: Detection threshold.\n",
        "    - hier_thresh: Hierarchical threshold.\n",
        "    - nms: Non-max suppression threshold.\n",
        "\n",
        "    Returns:\n",
        "    - A sorted list of detection results, each containing the class name, probability, and bounding box coordinates.\n",
        "    \"\"\"\n",
        "    im = lib.load_image_color(image.encode(\"utf-8\"), 0, 0)\n",
        "    num = c_int(0)\n",
        "    pnum = pointer(num)\n",
        "    lib.network_predict_image(net, im)\n",
        "    dets = lib.get_network_boxes(net, im.w, im.h, thresh, hier_thresh, None, 0, pnum)\n",
        "    num = pnum[0]\n",
        "\n",
        "    if nms:\n",
        "        lib.do_nms_obj(dets, num, meta.classes, nms)\n",
        "\n",
        "    results = []\n",
        "    for j in range(num):\n",
        "        for i in range(meta.classes):\n",
        "            if dets[j].prob[i] > 0:\n",
        "                b = dets[j].bbox\n",
        "                results.append((meta.names[i], dets[j].prob[i], (b.x, b.y, b.w, b.h)))\n",
        "\n",
        "    lib.free_image(im)\n",
        "    lib.free_detections(dets, num)\n",
        "\n",
        "    return sorted(results, key=lambda x: -x[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH2oe1riZ1o0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Function to process images listed in the validation files and save YOLO detection results.\n",
        "\n",
        "This function performs the following steps:\n",
        "1. Initializes the YOLO network and metadata.\n",
        "2. Reads validation image paths from 'val_from_test.txt' and 'val_from_train.txt'.\n",
        "3. Processes each image in the validation set to perform object detection.\n",
        "4. Saves the detection results in the specified directories.\n",
        "\n",
        "The function uses the YOLO model to detect objects in the images and saves the detection results\n",
        "as binary images where detected regions are highlighted.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize YOLO network and metadata\n",
        "net = lib.load_network(b\"~/cfg/yolov3-fishclef.cfg\", b\"~/fishclef.weights\", 0)\n",
        "meta = lib.get_metadata(b\"~/cfg/fishclef.data\")\n",
        "\n",
        "# Directories to save YOLO detection results\n",
        "save_test_part = \"~/Test_dataset/yolo_test_part\"\n",
        "save_train_part = \"~/Test_dataset/yolo_train_part\"\n",
        "\n",
        "# Read validation image paths\n",
        "with open(\"~/val_from_test.txt\") as val_from_test, open(\n",
        "    \"~/val_from_train.txt\"\n",
        ") as val_from_train:\n",
        "    val_test = [line.rstrip() for line in val_from_test]\n",
        "    val_train = [line.rstrip() for line in val_from_train]\n",
        "\n",
        "# Image dimensions\n",
        "img_height, img_width = 640, 640\n",
        "test_count = 0\n",
        "detected_count = 0\n",
        "\n",
        "# Process each image in the validation set\n",
        "for img_name in val_test:\n",
        "    test_count += 1\n",
        "    print(f\"Processing {test_count}/{len(val_test)}: {img_name}\")\n",
        "    video_file = os.path.basename(os.path.dirname(img_name))\n",
        "    img_file = os.path.basename(img_name)\n",
        "\n",
        "    # Create directory to save results if it doesn't exist\n",
        "    save_path = join(save_test_part, video_file)\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # Perform detection\n",
        "    detections = detect(net, meta, img_name)\n",
        "    detected_blob_img = np.zeros((img_height, img_width), dtype=np.uint8)\n",
        "\n",
        "    if detections:\n",
        "        detected_count += 1\n",
        "        print(f\"Detected in frame {detected_count}/{test_count}\")\n",
        "        for fish_info in detections:\n",
        "            x, y, w, h = map(int, fish_info[2])\n",
        "            xmin, ymin = max(0, x - w // 2), max(0, y - h // 2)\n",
        "            xmax, ymax = min(img_width, x + w // 2), min(img_height, y + h // 2)\n",
        "\n",
        "            # Only consider detections with area less than 25600\n",
        "            if w * h < 25600:\n",
        "                detected_blob_img[ymin:ymax, xmin:xmax] = int(fish_info[1] * 255)\n",
        "\n",
        "    # Save the detection result image\n",
        "    cv2.imwrite(join(save_path, img_file), detected_blob_img)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "SPN4GsCpZC4W",
        "BOgY7NnkQO4S",
        "bB9eG7qbV6jl",
        "hfFX_FAOYTQt",
        "xdYij-suZLWQ",
        "xPNG9tgHZa18",
        "3NNPM9OqZoGw",
        "2GFUTgDpZwlJ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
